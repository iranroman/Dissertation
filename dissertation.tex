\documentclass{report}
\usepackage{suthesis}
\usepackage{amsmath}
\usepackage{graphicx}

\dept{Music}

\begin{document}
\title{Mathematical models of rhythm synchronization and anticipation}
\author{Iran R. Roman}
\principaladviser{Chris Chafe}
\firstreader{Edward W. Large}
\secondreader{Julius O. Smith}

\beforepreface

\prefacesection{Abstract}
When humans synchronize with a periodic stimulus, endogenous processes like neural delays and spontaneous rates can explain the systematic asynchronies observed between human movements and stimulus onsets. This dissertation presents two different models that explain how neural delays and spontaneous rates of movement affect human synchronization. Additionally, these models have been implemented with tensorflow 2, allowing for parameter optimization and general applications in signal processing algorithms.
The first model explains how, in metronome synchronization tasks, people tend to tap slightly before the metronome clicks. This anticipation tendency increases with longer stimulus periods of up to 3500ms, but is less pronounced in trained individuals like musicians compared to non-musicians. In non-biological systems, anticipation is observed between delayed-coupled systems. Therefore, the human anticipation tendency could be explained with such a system because delayed communication is inherent to the sensorimotor system during perception-action coordination. This dissertation tests this hypothesis with a dynamical systems model consisting of an oscillator receiving its own delayed activity as input. Simulation experiments were conducted using previously published behavioral data from human studies with either synchronization to a metronome or interpersonal synchronization. Our new model is validated by its ability to simulate real human synchronization and anticipation data. As a result, our model informs theories of adaptive human synchronization.
The second model explains how interpersonal synchronization is affected by an individual’s spontaneous rates of movement. Specifically, the greater discrepancy between two synchronizing musicians' spontaneous rates, the greater asynchronies observed during joint duet performance. Interestingly, a musician's spontaneous rate remains stable after experiencing a joint performance, suggesting short-term tempo adaptation during joint performance and spontaneous rate restoration afterwards. This dissertation tests whether an oscillatory dynamical system with frequency elasticity and hebbian learning can explain how spontaneous rates affect interpersonal synchronization. The model consists of an oscillator with a natural frequency that emulates the human spontaneous rate. The oscillator’s frequency term is elastic to allow for short-term frequency changes during synchronization with a periodic stimulus of an arbitrary frequency. However, elasticity makes the oscillator return to its original natural frequency when it is no longer stimulated. Our model is validated by its ability to simulate human synchronization data using its adaptive and elastic frequency learning mechanism. This model can simulate duet musical performance and explain how asynchronies between performers are systematically influenced by the difference between two performers’ spontaneous rates.
Finally, this dissertation presents a novel implementation of these oscillatory models in tensorflow 2. This toolbox is written with a broad user-base in mind, and it includes general numerical methods for integration of ordinary differential equations. Besides allowing users to simulate different types of neural oscillators in multi-scale networks, the toolbox also allows oscillatory models to be combined with deep learning networks for the first time. Oscillatory networks are a new alternative for time-frequency analysis in deep learning algorithms, and can improve performance in common signal processing tasks.

\prefacesection{Acknowledgments}
I would like to thank my colleagues, family, and friends. Thank you to my CCRMA cohort of Ph.D. candidates: Kitty Shi and Tim O'Brien. Similarly, I want to thank all the friends I had at CCRMA: Madeline Huberth, Alex Chechile, Wisam Reid, Shu Yu Lin, Nick Gang, Ethan Geller, Rahul Agnihotri, Emily Graber, John Granzow, Woody Herman, Kai-Chieh Huang, Jay Kadis, Sasha Leitman, Sara Martı́n, Romain Michon, Kitty Shi, Dave Kerr, Fernando Lopez-Lezcano, Carr Wilkerson, Elliot Kermit-Canfield, Eoin Callery, Jorge Herrera, Rob Hamilton, Chryssie Nanou, Christopher Jette, Matt Wright, Nette Worthey, Mario Champagne, Velda Williams, and Debbie Barney. Outside of CCRMA, I want to thank the Bioscience community: Samar Fahmy, Terrance Mayes, Tim Stearns, Ayodele Thomas, and Tony Ricci. 
A big thank you to the Stanford Human-Centered Artificial Intelligence initiative for funding the last year of my Ph.D. studies, and the Howard Hughes Medical Institute for funding my first academic quarter at Stanford. 
I also want to thank CCRMA faculty. Thank you Malcolm Slaney and Takako Fujioka for providing critical feedback and advice to my modeling work presented in this dissertation. Thanks for Jonathan Abel and David Berners for letting me serve as their teaching assistant of signal processing. Thank you Jonathan Berger and Jaroslaw Kapuscinski for always being supportive and providing me with general academic advice. Thank you Ge Wang for your courses where I learned the fundamentals of algorithms applied to digital sound. 
My dissertation committee deserves a big thank you for all their work guiding my Ph.D. research. Thank you Julius Smith for inspiring me to always extend my horizons and for being an excellent role model of scholarship and creative thinking. Thank you Edward Large for always being supportive of my research efforts and guiding me whenever I felt lost. And thank you to Chris Chafe for trusting in my ideas and letting me shape my research agenda. 
My journey at Stanford was possible thanks to the support that my family gave me. Thank you Alejandra Torres for being the most caring and sincere individual I have ever met, and thank you for always being there. Thank you Iran A. Roman for bringing joy to all my days. I also want to thank my supportive and loving parents: Adriana Guzman Guzman and Iran Manuel Roman Jaimes. Finally, thank you to my brothers Rodolfo, Rodrigo, and Adrian.
\afterpreface

\chapter{Introduction}
\chaptermark{Introduction}

Humans carry out periodic actions everyday. Some of these actions are spontaneously paced, like walking, blinking, or swimming. Others are paced by external factors, like music when we dance, or other individuals when we perform crew rowing or play in a musical ensemble. Periodic actions are ubiquitous in human behavior, and they are an integral part of the most basic human functions (i.e. breathing, chewing, running) and the most complex social interactions (i.e. music making, military marching, or turn-taking conversing).

Every person has a different walking or blinking speed. This suggests that spontaneous periodic actions are influenced by personal factors, like anatomy and function of the body \cite{goodman2000advantages} and the brain \cite{latash1992virtual}, which in turn are influenced by one’s genetics and the environment. These internal factors seem to be flexible though, allowing a person to carry out periodic actions at different rates when synchronizing movements with another person or with an external stimulus like music or a metronome. 

Previous research of human periodic actions has identified very clear behavioral patterns. For example, while synchronizing with a simple periodic stimulus, like a metronome, humans show a tendency to anticipate the stimulus with their actions \cite{repp2007tapping}. Additionally, the difference between a person’s spontaneous motor tempo (SMT) and the stimulus tempo determines whether a person will anticipate or lag such a stimulus when synchronizing with it \cite{scheurich2018tapping}. Finally, when spontaneously performing a periodic action at a tempo different than the SMT, humans tend to drift back to the SMT \cite{zamm2018musicians}. 

These patterns and phenomena observed across different behavioral experiments suggest that there are very clear biophysical mechanisms at play when humans carry out periodic actions. However, currently there are conflicting views between models trying to explain the computational and biological origins of periodic action, synchronization, and anticipation. This dissertation aims at explaining how internal and external factors affect human periodic action using simple dynamical systems built with simple biophysical principles. 

\section{Summary of chapters}
This dissertation presents simple mathematical models, consisting of oscillatory equations with biophysical mechanisms, that explain human periodic synchronization and anticipation. One chapter of this dissertation also describes how these models can be combined to build a more general and robust model of human periodic action. Additionally, the tools developed to simulate these mathematical models serve as the foundation for future research using these oscillatory models to simulate human behavior and other biophysical phenomena. Finally, we also describe how these oscillatory equations can be used for signal processing, and as building blocks in artificial intelligence algorithms like deep neural networks.

\subsection{Chapter 1}
Here, in this introduction, we discuss the scope and structure of this dissertation, as well as previous modeling work trying to explain human periodic synchronization and anticipation.

\subsection{Chapter 2}
The first model presented in this dissertation explains why humans have a tendency to anticipate periodic events. When an individual synchronizes a body movement (i.e. finger taps) with a metronome, the individual’s actions tend to precede the metronome clicks. This anticipation tendency is observed for metronome inter-onset-intervals lengths ranging from 500ms to 3500ms \cite{repp2005sensorimotor}, and tends to be smaller among musicians than among non-musicians \cite{repp2007tapping}. Interestingly, similar anticipation tendencies have been observed in the field of physics when a system with delayed feedback synchronizes with a periodic signal (i.e. two coupled lasers where one of the lasers receives its own delayed output as input) \cite{stepp2010strong}. Considering this, we hypothesized that the human anticipation tendency is the result of delayed neural signals traveling back and forth between an individual’s motor effectors (i.e. finger tips) and the motor cortex. To test this hypothesis, we designed a mathematical model consisting of an oscillator with delayed feedback. The oscillator emulates periodic motor activity, and the delayed feedback simulates the axonal communication in the brain’s motor system. When stimulating this model with the same metronome rates that humans synchronize with, the model showed the same anticipatory tendencies observed in musicians. Additionally, by increasing the delayed feedback amplitude we observed the anticipatory tendency observed in non-musicians. To validate the model, we simulated multiple musical performance tasks where the anticipatory tendency has been observed in humans. Finally, the model can also make predictions about how humans will synchronize in other tasks, including solo and interpersonal synchronization settings.

\subsection{Chapter 3}
The second model explains how an individual’s spontaneous motor tempo (SMT) affects synchronization. Humans can spontaneously perform a simple melody showing an SMT. Every person’s SMT is determined by body anatomy and the brain's central pattern generators, and is consistent throughout adulthood \cite{scheurich2018tapping}. While humans can synchronize their movements with stimuli at different frequencies, behavioral research shows that the SMT systematically affects how an individual synchronizes. For example, individuals have a tendency to anticipate a stimulus frequency slower than the SMT, and lag a stimulus frequency faster than the SMT \cite{scheurich2018tapping}. This behavioral data suggests that humans can adaptively synchronize with an arbitrary stimulus tempo, but that the SMT acts as a constant pulling force. Considering this, we hypothesized that this phenomena can be explained with two biophysical mechanisms: Hebbian learning for tempo adaptation and elasticity to explain the SMT as a pulling force. To test this hypothesis, we designed a mathematical model consisting of an oscillator with Hebbian learning and elasticity that control the oscillator's frequency. This model simulates the brain's ability to synchronize neural activity with the period of an external stimulus while keeping the SMT in the absence of a stimulus. We used this model to simulate music performance tasks originally designed to study how the SMT affects human synchronization with a metronome or between musicians. The model was able to simulate the patterns observed in human data across different music performance tasks and was also able to make predictions that can be tested by collecting new human data.

\subsection{Chapter 4}
This chapter describes how the models described in chapters 2 and 3 can be combined to create a single, parsimonious, and biophysical model of human synchronization. Because the models are built with simple biophysical principles and validated with real human data, combining them will result in a model that will generalize across periodic action and/or synchronization tasks that humans can conceivably carry out. This new model will be able to make more predictions about human behavior, and should be validated with new human data.

\subsection{Chapter 5}
The last chapter of this dissertation describes a new software toolbox that was written to build and optimize oscillatory models using automatic differentiation. This toolbox was implemented in tensorflow 2, which was originally written to develop artificial intelligence algorithms and is the most advanced tool to build large network models. This toolbox allows the user, for the first time, to optimize oscillator models using human data targets. It also allows for future research using biophysical models as building blocks in artificial intelligence algorithms. This last chapter also presents an experiment where a biophysical model or neural oscillators is used to clean noisy speech. This proof of concept expands the possible ways to optimize oscillatory models beyond modeling of human behavior. 

\subsection{Chapter 6}
This chapter is the concluson of this dissertation. It summarizes the main findings of each chapter and the contributions of this dissertation.

\section{Literature review}

Every chapter in this document reviews the relevant literature. In this introductory chapter we give a broad overview of previous models of human anticipation and synchronization.

\subsection{Models of anticipation}
Humans have a natural tendency to anticipate events. A general definition of anticipation is “the forecast of a future event”. From a statistical perspective, anticipation involves previous knowledge about the likelihood of an event. Hence, anticipation in the statistical sense is inference from previous knowledge or evidence \cite{cox2006principles}. Karl Friston and colleagues have suggested that the brain carries out a complex and explicit mechanism of anticipation based on statistical inference \cite{maffei2017perceptual}. In their view, anticipation is a human cognitive process that involves statistical analysis of multisensory cues with the explicit goal of anticipating an event. Consistent with this perspective, multiple models of human anticipation carry out probabilistic computations on inputs. Examples include a Bayesian model of sensorimotor integration during movement learning \cite{kording2004bayesian}, and a recurrent neural network model of human anticipation and synchronization with a robot \cite{schydlo2018anticipation}. 

However, anticipation may be the result of simpler biomechanical properties. In the specific case of the anticipatory tendency observed when humans synchronize finger taps with a metronome, anticipation could be explained by the time difference in communication between the auditory and peripheral motor systems \cite{aschersleben2002temporal}. It takes less time for the metronome sounds to reach the auditory cortex than the fingertip feedback to reach the motor cortex. As a result, taps must anticipate metronome clicks so that the fingertip feedback and the metronome clicks reach the brain’s multisensory association areas at the same time \cite{aschersleben2002temporal}. This simple model can explain anticipation by considering the nervous system anatomy without the need for complicated and explicit statistical computations. The model presented in chapter 2 of this dissertation elaborates on the idea of delayed neural communication as the main cause of the anticipatory tendency observed when humans synchronize movements with a metronome. 

\subsection{Strong anticipation}
The kind of anticipation that emerges in a system as a result of its structure and function is known as 'strong anticipation'\cite{dubois2001incursive}. If anticipation emerges in a system as a result of statistical inference, then the system is carrying out 'weak anticipation' \cite{dubois2001incursive}. Stepp and Turvey \cite{stepp2010strong} emphasized that in neuroscience, anticipation is assumed to be 'weak anticipation' and most models ignore the structure and function of anticipatory systems. Because anticipation is observed even in non-living systems (i.e. systems unable to carry out statistical inference), assuming that human anticipation is the result of 'weak anticipation' is a limited perspective. Stepp and Turvey \cite{stepp2010strong} have demonstrated that anticipation, in general, can be observed in delayed coupled systems (i.e. a system receiving both a stimulus and its own delayed activity as input). When humans synchronize with a metronome, neural signals must travel back and forth across axons as long as 1 meter in length. So the anticipatory tendency observed during such a task can be explained with a delayed coupled system.

\subsection{Models of SMT}
The spontaneous motor tempo (SMT) can be measured by asking an individual to spontaneously move periodically, and seems to be stable throughout adulthood \cite{scheurich2018tapping}. There is debate around the mechanistic origins of the SMT, with some arguing that the SMT is determined by the rhythms of central pattern generators in each individual's brain \cite{latash1992virtual}, and others that the SMT is determined by the body’s anatomy and biomechanics \cite{goodman2000advantages}. Behavioral data has not favored one of these two hypothetical perspectives over the other. As a result, the field continues debating about the origins of the SMT, and not many modeling efforts have been developed to explain the mechanisms of the SMT. The model presented in chapter 2 uses elasticity to explain the SMT as an attractor state in a physical system with optimal energy use. This perspective is compatible with both the anatomy and central pattern generator origins of the SMT. 

\subsection{Models of lead and lag} 
Human actions do not always anticipate (i.e. lead) the onset of the pacing signal they are trying to synchronize with. If a transmission delay exists between the pacing signal and a synchronizing human, the anticipation tendency can be completely canceled out and instead a lag will be observed. Chafe and colleagues studied this in a controlled setting where they manipulated the length of a transmission delay between two synchronizing musicians to observe this transition from lead to lag in human synchronization behavior \cite{chafe2010effect}. Gurevish \cite{gurevish2004simulation} simulated this phenomena with a simple arithmetic model, and Caceres \cite{caceres2013synchronization} simulated it with an oscillator model. The arithmetic model is capable of perfectly identifying the tempo in the absence of delay, but when the delay is present it underestimates or overestimates the tempo, resulting in lead and lag, respectively \cite{gurevish2004simulation}. The oscillatory model leverages the oscillator's phase-locking dynamics to explain the lead and lag \cite{caceres2013synchronization}. An oscillator will have a phase-locking regime around its natural frequency and the frequency difference between the oscillator natural frequency and the stimulus frequency will determine the phase difference \cite{kim2015signal}. Hence, if the stimulus is faster that the oscillator, the phase difference will be positive (i.e. the oscillator will lag the stimulus), and if the stimulus is slower than the oscillator, the phase difference will be negative (i.e. the oscillator will lead the stimulus). However, oscillators usually have a very limited phase-locking regime around their natural frequency, so their ability to simulate lead and lag dynamics is limited to stimulus frequencies that are very close to the oscillator natural frequency \cite{kim2015signal}. In chapter 3 we describe how Hebbian frequency learning can help oscillators overcome this limitation. 


\chapter{Delayed feedback embedded in perception-action coordination cycles results in anticipation behavior}
\chaptermark{Delayed feedback results in anticipation behavior}

\section{Abstract}
Dancing and playing music require people to coordinate actions with auditory rhythms. In laboratory perception-action coordination tasks, people are asked to synchronize taps with a metronome. When synchronizing with a metronome, people tend to anticipate stimulus onsets, tapping slightly before the stimulus. The anticipation tendency increases with longer stimulus periods of up to 3500ms, but is less pronounced in trained individuals like musicians compared to non-musicians. Furthermore, external factors influence the timing of tapping. These factors include the presence of auditory feedback from one’s own taps, the presence of a partner performing coordinated joint tapping, and transmission latencies (TLs) between coordinating partners. Phenomena like the anticipation tendency can be explained by delay-coupled systems, which may be inherent to the sensorimotor system during perception-action coordination. Here we tested whether a dynamical systems model based on this hypothesis reproduces observed patterns of human synchronization. We simulated behavior with a model consisting of an oscillator receiving its own delayed activity as input. Three simulation experiments were conducted using previously-published behavioral data from 1) simple tapping, 2) two-person alternating beat-tapping, and 3) two-person alternating rhythm-clapping in the presence of a range of constant auditory TLs. In Experiment 1, our model replicated the larger anticipation observed for longer stimulus intervals and adjusting the amplitude of the delayed feedback reproduced the difference between musicians and non-musicians. In Experiment 2, by connecting two models we replicated the smaller anticipation observed in human joint tapping with bi-directional auditory feedback compared to joint tapping without feedback. In Experiment 3, we varied TLs between two models alternately receiving signals from one another. Results showed reciprocal lags at points of alternation, consistent with behavioral patterns. Overall, our model explains various anticipatory behaviors, and has potential to inform theories of adaptive human synchronization.

\section{Introduction}
In social settings, people must coordinate actions in order to carry out fundamental activities like walking or talking. Some activities require individuals to precisely time repetitive actions such as dancing, rowing, or music making, resulting in synchronization with external information, shared among a group of individuals. This kind of perception-action coordination is also sometimes called sensorimotor synchronization, because it is considered to depend on communication between the sensory and motor areas of the nervous system \cite{oullier2005neural}. The simplest form of synchronization happens when individuals tap in synchrony with an isochronous stimulus. In doing so, individuals’ actions on average tend to slightly precede the stimulus, resulting in a mean negative asynchrony between the stimulus onsets and corresponding taps. This negative mean asynchrony has been observed consistently in the literature. Anticipation is observed when humans tap with an isochronous stimulus with inter-onset-intervals (IOIs) ranging from 300ms to 4800ms \cite{mates1994temporal}. However, the asynchronies vary widely and can be positive, on average, for an individual tapping with IOIs longer than 2000ms \cite{mates1994temporal}. For IOIs greater than 2000ms, asynchronies may show a bimodal distribution; some taps precede the stimulus while others follow it, with longer IOIs resulting in more taps that follow the stimulus and fewer that precede it \cite{baaaath2016estimating}. The anticipation tendency is influenced by musical experience, as tap timing in musicians is closer to the stimulus than that in non-musicians for IOIs between 1000ms and 3500ms \cite{repp2007tapping}. When the IOI is greater than 5000ms, more taps occur after the stimulus than before the stimulus, suggesting that people are more reactive upon hearing the next beat \cite{miyake2004two}. Similarly, when synchronizing with the beat underlying complex surface rhythms (e.g., syncopation), the mean asynchrony shifts to the positive side \cite{wohlschlager1999synchronization, snyder2001tapping, large2015neural} (see \cite{repp2005sensorimotor} for a review). Collectively, these results indicate that the anticipation tendency depends not only the IOI, but also, the expertise level, complexity of the rhythms and the task requirements.

Two or more people can also perform coordinated rhythmic behavior. For example, two musicians may alternate taps to maintain a common stable tempo. To achieve coordination, they must employ an interactive and adaptive strategy and adjust their tap timing based on their own timing as well as their partner's. The asynchrony of one person tends to copy the previous asynchrony produced by their partner \cite{nowicki2013mutual, konvalinka2010follow}. This tendency is not observed when one of the partners cannot hear the actions of the other, indicating that the auditory feedback between synchronizing partners can affect their ability to coordinate \cite{palmer201210}. It appears that in the presence of auditory feedback, coordination is not affected by the presence or absence of other non-auditory information \cite{nowicki2013mutual, marmelat2012strong}. Another factor that affects auditory information in coordination is transmission latency (TL), which refers to a delay between the time at which an event occurs and the time when the associated auditory information is available. TLs are a result of the transmission of information across a physical distance separating two synchronizing individuals \cite{chafe2010effect}. Analogous latencies can be observed when musicians at remote locations play music together over the internet. One study has examined the effect of the TL between two individuals trying to alternately clap a rhythm \cite{chafe2010effect}. The authors observed that stable synchronization is achieved with small TLs (10-20ms), while musicians collectively speed up for TLs shorter than 10ms, and slow down for latencies longer than 20ms \cite{chafe2010effect}.

Different models try to explain how neural computations give rise to anticipation and coordination. Researchers have proposed that the anticipation results from the combination of information from different modalities, based on differences in the axonal distances between the hand, the ear, and the brain \cite{aschersleben1995synchronizing, prinz1992don}. For example, the sensory accumulation model \cite{aschersleben2002temporal} proposes that the time difference in central-peripheral signal communications between the auditory and motor systems may be responsible for the anticipation; it takes less time for pacing stimuli to travel from the ear to the auditory cortex compared to the time it takes a signal to travel from the fingertips to the brain and vice versa. As a result, taps precede external auditory signals so somatosensory and auditory information will coincide at the level of the central nervous system. One prediction of the sensory accumulation model is that louder stimuli will result in smaller anticipation. Białuńska, Bella, \& Jaśkowski tested this theory behaviorally, and found that the prediction was not confirmed, indicating that other sensorimotor mechanisms must be involved \cite{bialunska2011increasing}. Another model proposes perceptual underestimation of IOIs \cite{flach2005transition}. This predicts that anticipation should be reduced when periodic stimuli are subdivided into equidistant strong and weak beats \cite{loehr2009subdividing, repp2008metrical, zendel2011effects}. However, due to conflicting results, an integrative and convergent explanation is still yet to be established (see \cite{repp2013sensorimotor} for a review).

Mechanisms underlying generalized anticipatory behavior beyond the simple anticipatory phenomena have recently been of interest. Dubois \cite{dubois2001incursive} has identified two main theories as to how anticipatory behavior arises. The first one is called ‘weak anticipation theory’, proposing that anticipation occurs as the result of inferences produced by internal models. Specifically, this theory argues that because the brain generates representations of likely future events based on external information, anticipation is the result of the brain’s eagerness to confirm the correctness of these representations \cite{clark1998being, clark1999towards}. The second perspective is termed ‘strong anticipation theory’, which suggests that anticipation results from the homeostatic coupling of information between an organism and its environment. Stepp and Turvey \cite{stepp2010strong} note that, in its original Latin meaning, anticipation refers to the act of “following a path before.” Accordingly, anticipation must involve not only correctly predicting another model’s actions but also realizing that prediction with one’s own actions \cite{stepp2010strong}. Stepp and Turvey also explained that homeostatic mechanisms can keep an anticipating organism in a proactive state \cite{stepp2010strong}. One can see that a major drawback of the weak anticipation theory is that it cannot explain how some physical systems anticipate, even without the ability to carry out inference (e.g., laser semiconductors and electronic circuits) \cite{washburn2015harmony, kelso1995dynamic}. In contrast, strong anticipation theory can explain anticipation as a theoretical framework that generalizes across physical systems \cite{stepp2010strong}. The present study is aimed at exploring how the strong anticipation theory could further explain various results in rhythmic coordination in an integrative manner, including anticipatory synchronization, by conducting computational simulations.

Specifically, 'anticipatory synchronization' in strong anticipation emerges from the coupling between a 'response' system and a 'driver' system (e.g., stimulus input) wherein the response system also receives delayed feedback about its own activity. One of the major strengths of the strong anticipation approach is that it accounts for anticipatory phenomena beyond human behavior, and that collectively all such phenomena can be modeled as coupled dynamical systems \cite{stepp2010strong, washburn2015harmony, kelso1995dynamic}. There are parallels between a dynamical system with delayed feedback and the delayed communication between different areas in the sensorimotor system of humans carrying out synchronization. Synchronization requires communication between auditory, premotor and motor brain areas \cite{merchant2015finding, banerjee2007neural, slowinski2016effects}, which involves delayed transmission of neural information. Moreover, it has been suggested that these delays inherent within the human sensorimotor system may act like ones in coupled dynamical systems with delayed feedback inputs \cite{washburn2015harmony, banerjee2007neural, slowinski2016effects}, supporting the strong anticipation theory. If this holds true, a low-dimensional dynamical systems model could explain anticipation in perception-action coordination.

To simulate periodic perception-action coordination, we use an oscillator described by Eq \eqref{eq:2.4} (see model definition in the methods section). This oscillator can synchronize with periodic external stimuli, a feature that has been exploited by a model capable of beat tracking in complex musical rhythms \cite{large2015neural, velasco2011pulse}. The oscillator’s periodic activity phase-locks with external periodic stimuli close to its fundamental frequency and also at integer-ratio relationships \cite{large2010canonical}.

In contrast to the models previously described by Large and colleagues using a network of coupled oscillators \cite{large2015neural, velasco2011pulse, large2010canonical}, our model consists of a single oscillator. To simulate periodic synchronization, we add delayed recurrent feedback to this single oscillator. Such delayed recurrent feedback is essential for a model of synchronization, as no neural process is instantaneous \cite{banerjee2007neural}. We refer to our model as the Strong Anticipation in Periodic Perception Action (SAPPA) model (see model definition in the methods section). Below we describe three simulation experiments (see Fig \ref{f2_1}) based on published data corresponding to three distinct behavioral studies.

\begin{figure}
    \centering
    \includegraphics[width=0.9\textwidth]{figures/fig2_1.png}
    \label{f2_1}
\end{figure}
\begin{figure}[t]
    \caption[Illustration of the synchronization tasks and corresponding simulation experiments]{\textbf{Illustration of the synchronization tasks and corresponding simulation experiments.} (A) The task simulated in Experiment 1, in which a person synchronizes with a metronome (top). Illustration of our simulation, in which our model synchronizes with an external sinusoidal stimulus (bottom). (B) The first task simulated in Experiment 2, in which one musician taps to every other metronome beat while listening his or her own taps (top). Illustration of our simulation in which a SAPPA model synchronizes with an external sinusoidal stimulus (bottom). Blue colored part of the model’s activity indicates that the model is receiving its own non-delayed activity as input in addition to the external sinusoid, and gray colored part indicates that the model only receives the external sinusoid as input. (C) The second task simulated in Experiment 2. This task is the same as the first one described in (B), except that the musician did not hear his or her own taps (top). Illustration of our corresponding simulation (bottom). The gray lines indicate that the model only receives the external sinusoid as input. (D) The third task simulated in Experiment 2, in which two musicians alternately tap with a metronome while listening to their own taps and the other musician’s taps (top). Illustration of our simulation where two models synchronize with an external sinusoidal stimulus (bottom). Blue and red colored parts of the model’s activity indicate the time window where the model’s non-delayed activity is used as input for both models in addition to the external sinusoid, while grayed part indicates the time window when the model receives the non-delayed activity of the other model in addition to the external sinusoid as input. (E) The fourth task simulated in Experiment 2. This task is the same as the third one described in (D), except that the musicians did not hear their own or each other’s taps (top). Illustration of our corresponding simulation (bottom). The grayed cycles indicate that the models only receive the external sinusoid as input. (F) The task simulated in Experiment 3, in which two musicians clapped a rhythm alternately (top). Illustration of our simulation where two models oscillate while alternately receiving each other’s activity as input (bottom). Blue and red cycles indicate the model whose activity is received by both models as input, while gray cycles indicate that the model’s activity is not received as input by either model. TL stands for transmission latency.}
\end{figure}

In our first experiment, we use the SAPPA model to reproduce the data from a study in which musicians and non-musicians synchronized their tapping with an isochronous metronome across a wide range of tempi, representing the simplest form of synchronization task \cite{repp2007tapping} (Fig \ref{f2_1}A). Specifically, we hypothesize that the delayed recurrent feedback in the SAPPA model would result in an increasing anticipation tendency for the longer stimulus periods. Additionally, the smaller anticipatory tendency observed in musicians compared to non-musicians’ anticipatory tendency would be achieved by reducing the amplitude of the delayed recurrent feedback.

In our second experiment, we reproduce the data from a study in which two musicians alternately tapped to a metronome \cite{nowicki2013mutual}. We are specifically interested in the data from two tasks where musicians tapped at every second metronome beat alone (Fig \ref{f2_1}B and \ref{f2_1}C) or alternated this tapping with another musician as a duet (Fig \ref{f2_1}D and \ref{f2_1}E). In both cases, their performance was measured in two conditions, with and without auditory feedback from tapping. The results show a smaller anticipatory tendency, in both solo and duet settings, when musicians heard their own taps compared to when they did not. We hypothesized that the lack or presence of non-delayed feedback in our model can simulate auditory feedback conditions, affecting the size of the anticipation.

In our third experiment, we reproduce behavioral data from duet performance with TLs (varying from 3ms to 78ms for different trials) where two musicians alternately clapped the same rhythmic pattern without a metronome \cite{chafe2010effect} (Fig \ref{f2_1}F). Latencies longer than 20ms caused the musician starting the pattern to lag the musician finishing the pattern, while latencies shorter than 10ms caused the musician starting the pattern to clap ahead of the other musician’s last clap. This resulted in the collective tempo gradually slowing down for the longer TLs and speeding up for the shorter ones. We hypothesized that our model’s anticipation would be affected by longer TLs, resulting in lag between two synchronizing SAPPA models and a slowing tempo.

Perception-action coordination tasks capture how external and internal factors affect synchronization. The strong anticipation hypothesis explains synchronization with dynamical systems receiving external stimuli and delayed recurrent feedback. Our interest is to test whether a mathematical model of strong anticipation can be configured for solo and duet settings to perform a variety of synchronization tasks, and if so whether it could explain behavioral patterns observed in human data. This would demonstrate that the strong anticipation hypothesis accounts for complex biological phenomena like perception-action coordination.

\section{Results}

\subsection{Experiment 1: Individual tapping in synchronization with an \\ isochronous stimulus}

We simulated the model to represent an individual tapping with an isochronous stimulus (see Fig \ref{f2_1}A), resulting in an anticipatory tendency. Fig \ref{f2_2}A shows the linear regression we performed on the human data described by Repp and Doggett \cite{repp2007tapping}. We stimulated our model with a periodic external sinusoid and measured its anticipation (see model definition and parameter analysis in the methods section) with respect to the external sinusoid. In order to observe the relationship between anticipation and stimulus period length, we optimized our model parameters (see parameter analysis in the methods section) to simulate the anticipation shown by musicians and non-musicians, as a function of IOI \cite{repp2007tapping}. In Experiment 1, a single SAPPA model was stimulated by an external sinusoid while also receiving its own non-delayed activity as input. The range of oscillatory frequencies tested in Experiment 1 ranged from 0.29Hz to 1Hz (see the methods section for a full description of the model parameters and experimental conditions). This contrasted with Experiments 2 and 3, where the same model was stimulated differently, at a smaller set of frequencies.

\begin{figure}
    \centering
    \includegraphics[width=1.0\textwidth]{figures/fig2_2.png}
    \caption[Dynamical systems model of anticipation when musicians and non-musicians synchronize with an isochronous stimulus]{\textbf{Dynamical systems model of anticipation when musicians and non-musicians synchronize with an isochronous stimulus.} (A) The anticipation (mean values with error bars representing the standard error of the mean) in musicians and non-musicians tapping with an isochronous metronome while listening their own taps. The regression lines for the mean values are also shown. (B) The anticipation obtained when the musician (green dots) and non-musician (yellow dots) SAPPA models were stimulated by an external sinusoid while also receiving their own non-delayed activity as input ($A = -0.5$; see model definition in the methods section). (C) The anticipation obtained when the musician (gray-green dots) and non-musician (gray-yellow dots) SAPPA models were only stimulated by an external sinusoid and did not receive their own non-delayed activity as input ($A = 0$; see model definition in the methods section). In all simulations $\tau = 0.222$ seconds. The $D$ parameter differentiates the musician and non-musician models. The same regression lines for the behavioral data are shown in both (A), (B) and (C) for comparison purposes (see Supplementary Fig \ref{fS_1} for the model’s behavior with a square wave input).}
    \label{f2_2}
\end{figure}

We ran our simulations for stimuli with periods of 1000, 1250, 1500, 1750, 2000, 2250, 2500, 2750, 3000, 3250, and 3500ms. The SAPPA model always had a recurrent feedback delay with length of 222ms. To simulate the musician and non-musician data, the SAPPA model had $D = 0.05$ and $D = 0.36$, respectively, where the parameter $D$ is the amplitude of the delayed recurrent feedback in the SAPPA model (see model definition and parameter analysis in the methods section). Using the same recurrent feedback delay length, we were able to make the slope of the anticipation, as a function of IOI, more negative by increasing the amplitude of $D$. Fig \ref{f2_2}B shows how our simulated anticipatory tendencies align with the line of regression on the behavioral data for musicians and non-musicians in the study by Repp and Doggett \cite{repp2007tapping}. Fig \ref{f2_2}C uses the SAPPA model to predict how behavioral data would look if musicians and non-musicians carried out the same task only listening to the metronome and not receiving auditory feedback about their taps. Under this condition, the SAPPA model predicts that the asynchrony in musicians and non-musicians would become more negative, compared to when they received auditory feedback about their taps.

In this simulation, the following parameters were used: $A = -0.5$ (non-delayed feedback amplitude), $\tau = 0.222$ seconds (delay length), and $D = 0.05$ and $D = 0.36$ (delayed feedback amplitude) for the musician and non-musician SAPPA models, respectively. The stimulating sinusoid had an amplitude of $1$, while $A = -0.5$. This means that the model was forced more strongly by the external sinusoidal stimulus than by its own activity and that the SAPPA model’s phase-locking behavior was determined by the phase of the external sinusoid. Also, since recurrent feedback terms were negative ($A = -0.5$), the delayed recurrent feedback shifted the phase of $F$ in a negative direction with respect to $\exp(i2\pi f_s t)$. This means that the SAPPA model’s actions were more aligned with $F = \frac{\exp(i2\pi f_s t)+Az}{|\exp(i2\pi f_s t)+Az|}$, thus resulting in reduced anticipation compared to when $F = \exp(i2\pi f_s t)$ (see the methods section for a thorough description of the model’s dynamics, parameters, and discussion of how different model components interact with each other).

\subsection{Experiment 2: Interpersonal synchronization during alternating \\ paced tapping with or without auditory feedback}

Experiment 1 showed that our model can capture the anticipation observed in musicians and non-musicians in a simple metronome tapping task. Next, we examined how the SAPPA model could perform the more complex tasks in the study by Nowicki and colleagues \cite{nowicki2013mutual}. This task was carried out by solo musicians and also by musician duets. In the solo task, musicians tapped every other beat in synchrony with a metronome while hearing their own taps (feedback-on, Fig \ref{f2_1}B) or only hearing the metronome (feedback-off, Fig \ref{f2_1}C). In the duet task, pairs of musicians alternately tapped with a metronome while hearing their own and the partner’s taps (feedback-on, Fig \ref{f2_1}D) and also while only hearing the metronome (feedback-off, Fig \ref{f2_1}E). The same model and parameters used in Experiment 1 were used in Experiment 2, except for the f term, which was varied in Experiment 1 but was always 1Hz in Experiment 2, matching the frequency of the stimulating external sinusoid. Another major difference was the way in which the model was stimulated. Similar to Experiment 1, each SAPPA model in Experiment 2 was constantly stimulated by an external sinusoid. However, in Experiment 2 in the solo condition with feedback-on, the model received its own non-delayed activity as input only during half of each cycle. Moreover, in Experiment 2 in the duet condition with feedback-on each model received its own non-delayed activity as input during one half of each cycle and the other model’s non-delayed activity during the other half of the cycle. Finally, in Experiment 2 in the feedback-off condition, the models were only stimulated by the external sinusoid (see the methods section for a full description of the model parameters and experimental conditions).

Fig \ref{f2_3}A shows that the anticipation exhibited by musicians when solo-tapping on every other beat to an isochronous metronome was larger without auditory feedback from their own taps. The results of our solo-tapping simulations for the musician and non-musician SAPPA models are illustrated in Fig \ref{f2_3}B and \ref{f2_3}C, respectively (the SAPPA models used in this simulation have the same parameters found in Experiment 1; see the methods section for details about how we carried out this simulation of the solo task). We observed that the SAPPA model’s anticipation was smaller when the model did receive its non-delayed activity as input, compared to when only receiving the external sinusoid as input. While no study has yet investigated non-musicians’ asynchronies in this solo-tapping task, our non-musician SAPPA model predicts that non-musicians’ asynchronies would be larger than musicians’ asynchronies. Our SAPPA model also predicts that, similar to musicians, the non-musicians’ asynchronies will be smaller when individuals can hear their own taps along with the metronome, compared to when they only hear the metronome.

\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{figures/fig2_3.png}
    \caption[The effect of auditory feedback on anticipation when musicians synchronize with an isochronous metronome alone or with a musician partner]{\textbf{The effect of auditory feedback on anticipation when musicians synchronize with an isochronous metronome alone or with a musician partner.} (A) Behavioral measurements when a single musician taps every other beat in synchrony with a metronome for the feedback-on and feedback-off conditions (mean asynchronies with error bars representing the standard error of the mean). (B) The musician SAPPA model’s anticipation when synchronizing with an external sinusoid, while receiving (feedback-on) or not receiving (feedback-off) its non-delayed activity as input every other beat. (C) The non-musician SAPPA model’s anticipation when synchronizing with an external sinusoid, while receiving (feedback-on) or not receiving (feedback-off) its non-delayed activity as input every other beat. Dotted contours around circle data points indicate that this is a prediction that can be tested with behavioral data from non-musicians. (D) Behavioral measurements when two musicians tap every other beat in synchrony with a metronome for feedback-on and feedback-off conditions (mean asynchronies with error bars representing the standard error of the mean). (E) The anticipation when two musician SAPPA models synchronize with an external sinusoid, while alternating (feedback-on) or not receiving at all (feedback-off) each other’s non-delayed activity as input every beat. (F) The anticipation when two non-musician SAPPA models synchronize with an external sinusoid, while alternating (feedback-on) or not receiving at all (feedback-off) each other’s non-delayed activity as input every beat. Dotted borders around data points indicate that this is a prediction that can be tested by collecting behavioral data from non-musicians.}
    \label{f2_3}
\end{figure}

Fig \ref{f2_1}D and \ref{f2_1}E show the duet behavioral task, which consists of two musicians tapping in alternation by each synchronizing with every other beat of an isochronous metronome. Fig \ref{f2_3}D shows that, similar to the solo task results, the anticipation in musicians for the duet task was larger when they could not hear their own or the partner’s actions. The results of our duet-task simulations with the musician and the non-musician SAPPA model are illustrated in Fig \ref{f2_3}E and Fig \ref{f2_3}F, respectively (the SAPPA models used in this simulation have the same parameters found in Experiment 1; see the methods section for details about how we carried out this simulation of the duet task). We observed that the SAPPA model’s anticipation was smaller when models alternatingly received each other’s non-delayed activity as input, compared to when they only received the external sinusoid as stimulus. No study has yet investigated non-musicians’ asynchronies in this duet-tapping task, but we can use the non-musician SAPPA model to make predictions. Compared to anticipation in musicians, the SAPPA model predicts that anticipation in non-musicians would be larger. The SAPPA model also predicts that, similar to musicians, the non-musician anticipation will be smaller when pairs of individuals can hear each other’s alternating taps along with the metronome, compared to when they only hear the metronome.

\subsection{Experiment 3: Interpersonal synchronization during rhythm-\\ clapping alternation in the presence of transmission latencies}

In Experiment 2 we coupled pairs of SAPPA models to simulate the flow of auditory information between partners performing a paced alternating tapping task. Here we conduct another simulation for the data described by Chafe and colleagues \cite{chafe2010effect}, by further extending our model and task configuration. Their study examined how TLs between duet partners affected synchronization while they alternately clapped a rhythmic pattern (see Fig \ref{f2_1}F). The two novel features in their task were that they added TLs to the information exchange between duet partners who performed the task in two different rooms, and that they used a rhythmic pattern for alternating clapping instead of single-beat alternation. The same model and parameters used in Experiments 1 and 2 were used in Experiment 3. The main differences were, again, the way in which the model was stimulated and the oscillatory frequency. In contrast with Experiments 1 and 2, the SAPPA models in Experiment 3 were never stimulated by an external sinusoid. Instead, pairs of oscillators ($f = 1.5$Hz) stimulated each other in an alternating fashion every cycle (see the methods section for a full description of the model parameters and experimental conditions).

In the results by Chafe et al. \cite{chafe2010effect}, for TLs smaller than 10ms, the musician starting their rhythm showed a tendency to slightly lead the musician finishing a turn. With TLs greater than 10ms, the musician starting their turn lagged the musician finishing a turn. A similar pattern of behavior was observed as a result of TLs between the two oscillators. Fig \ref{f2_4}A illustrates our simulations with two oscillators that alternated cycles based on which oscillator served as the ‘active’ one. The colored background indicates the oscillator whose activity was used as the input for both oscillators (see the methods section for more details about how we carried out this simulation). In the example shown in Fig \ref{f2_4}A, the oscillator initiating its active cycle tended to lag the endpoint of the other oscillator as a result of the TL. In human data, these asynchrony measures grew as a function of TL, as shown in Fig \ref{f2_4}B. Fig \ref{f2_4}C and \ref{f2_4}D show our simulations with musician and non-musician SAPPA models, respectively, where the lag between coupled models showed a growth similar to the one observed in the behavioral study. The main difference between the behavioral data (Fig \ref{f2_4}B) and our simulations (Fig \ref{f2_4}C and \ref{f2_4}D) is that in our simulations TLs smaller than 10ms did not result in positive values. No existing studies have addressed how non-musicians perform in this task. Our non-musician SAPPA model predicts that, compared to musicians, the effect of TLs on non-musician synchronization behavior will be similar to that observed for musicians, indicating that TLs consistently affect synchronization between two clapping individuals, independent of musical expertise.

\begin{figure}
    \centering
    \includegraphics[width=1.0\textwidth]{figures/fig2_4.png}
    \caption[The effect of transmission latencies on the anticipation of pairs of musicians alternatively clapping a rhythm]{\textbf{The effect of transmission latencies on the anticipation of pairs of musicians alternatively clapping a rhythm.} (A) Illustration of the dynamics observed during our simulations in the presence of a TL between two synchronizing SAPPA models. The alternating blue and red background colors indicate which model’s activity is used as input to both models. The arrows show the end points of cycles for both models. Note how the passive model lags the active model in turn at the end of the cycle, due to the presence of the TL. (B) The lead and lag between musicians (measured as the percentage of a 90 bpm beat) as a function of TLs (mean values with error bars representing the 95\% variance), with the linear regression on the behavioral data. (C-D) The lead and lag between pairs of musician (C) and non-musician (D) models, with the linear regression from the behavioral data (B) for comparison purposes.} 
    \label{f2_4}
\end{figure}

\section{Discussion}

\subsection{Delayed recurrent feedback results in anticipatory \\ tendencies during paced periodic action}

Our model shows that asynchronies become smaller when the length of the recurrent feedback delay is shortened (see parameter analysis in the methods section). This is a result of the fact that $z$ and $z(t—\tau)$ become maximally aligned as $\tau$ shrinks. Consistent with the observations by Ciszak and colleagues \cite{}, our simulations show that the phase difference between $z$ and $z(t—\tau)$ is what causes anticipatory behavior. This behavior is similar across different types of external periodic inputs (for example, see Supplementary Fig \ref{fS_1} for the model’s behavior when the external input is a square wave instead of a sinusoid). It is interesting to consider whether the observed phase difference between $z$ and $z(t—\tau)$ is similar to the communication delays that exist in the physiology of the sensorimotor system. In principle, today's imaging techniques can support the quantification of actual conduction delays between neurons located in the auditory cortex, the basal nuclei, the cerebellum, premotor and motor cortex, as well as extremities and effectors (e.g., fingers). For example, high resolution neuroimaging methods and the physical models of axonal conduction delays may allow for the estimation of these delays through integration over populations \cite{swadlow2012axonal}. However, these areas contain vast numbers of neuronal connections which likely differ from each other in their functional pathways. Thus, detailed estimation of these delays at microscopic levels may not easily translate to macroscopic representation of neuronal population behaviors. Given that the recurrent feedback delay length is arbitrarily set in our model, the recurrent feedback delay length described here should be considered a representation of the potential function of neural transmission delays in a collective fashion.

\subsection{Experiment 1: Individual tapping in synchronization with an \\ isochronous stimulus}

In the SAPPA model Eq \eqref{eq:2.5}, a larger $D$ increases the amplitude of the delayed recurrent feedback. These different amplitudes have an effect on the anticipatory mechanisms of the model. $D$ is divided by the frequency $f$ in Hz. As $f$ becomes smaller, becomes exponentially larger. Hence, the delayed recurrent feedback is amplified as a function of the stimulus interval, which in turn results in the growing anticipation. Physiologically, this means that the parameter could indicate the amplitude of neural activity that one processes in addition to the external stimulus. This amplitude becomes larger as the stimulus becomes slower.

Our model was able to reproduce the mean anticipatory dynamics of musicians and non-musicians tapping with IOI periods between 1000ms and 3500ms. This was due to the smaller $D$ that made the musician SAPPA model anticipation curve have a less negative slope as a function of stimulus period length, compared to the larger $D$ that resulted in a more negative slope of non-musician SAPPA model. Since the amplitude of the delayed recurrent feedback in the non-musician SAPPA model is overall larger compared to the musician SAPPA model, makes the delayed recurrent feedback grow more in amplitude as a function of longer stimulus period. As discussed above, directly relating this $D$ parameter to the real neural processes might be difficult because it captures the cumulative effect of multiple interacting neural delay processes \cite{van2003self}. What we can say is that the larger $D$ value in the non-musician model causes larger amplitudes in the oscillators compared to the musician model.

We assumed that a linear regression on the behavioral data could fairly characterize the anticipatory dynamics observed in musician and non-musician data (Fig \ref{f2_2}A). This assumption is a limitation: anticipatory timing does not grow indefinitely as a function of IOI. Experiments where humans synchronize taps with a metronome of IOIs longer than 3500ms show that some taps precede the stimulus while others follow it, reducing the mean asynchrony \cite{baaaath2016estimating}, or even showing mean positive asynchronies for IOIs greater than 5000ms \cite{miyake2004two}. Hence, the relationship between metronome IOI and human asynchronies are clearly non-linear. Non-linearities are a common feature in systems with delayed recurrent feedback \cite{khalil2002nonlinear}. As a result, our model’s behavior is non-linear, a feature that is more obvious in our simulation of non-musician anticipation than musician anticipation (Fig \ref{f2_2}B and \ref{f2_2}C). These non-linear features could be used to expand the SAPPA model by making it show positive asynchronies in simple tapping tasks, like people do when stimulated by IOIs greater than 5000ms \cite{miyake2004two}. Because of these non-linear features, the SAPPA model has potential to explain both positive and negative asynchronies observed when people tap with metronomes of different IOI lengths.

Behavioral studies have consistently shown that anticipation when tapping with a metronome differs between musicians and non-musicians \cite{mates1994temporal, repp2007tapping, miyake2004two}. Our model offers an explanation as to what underlying mechanisms could give rise to these differences. Here, only the $D$ parameter was different between our musician and non-musician models, with $D$ being smaller in the musician model. Moreover, the simulation in Fig \ref{f2_2}C predicts that the anticipation will be larger for both musicians and non-musicians synchronizing with a metronome without listening their own taps, compared to when they listened their own taps (Fig \ref{f2_2}A and simulation in Fig \ref{f2_2}B). This prediction made by the SAPPA model could be tested by collecting empirical evidence from musicians and non-musicians. Furthermore, by changing its parameters, our model allows us to make even more predictions, beyond the ones made in Fig \ref{f2_2}C.

Compared to the musician model, the larger $D$ parameter in the non-musician model could be equivalent to a scenario where non-musicians listen to their internal processes more than musicians. In other words, in the non-musician’s brain the strength of internal feedback is larger compared to the musician’s brain, resulting in processing of an internal signal in addition to processing of external stimuli or behavioral adjustments. In this analysis we focused on the $D$ parameter, but the $\beta$ parameter in the SAPPA model would have had a similar effect (see model definition in the methods section for a full description of the model parameters). Electrophysiological findings from investigations on cortical oscillatory amplitude support this observation. Compared to resting conditions, non-musicians listening to music show larger oscillatory amplitudes only in the delta band (between 0.5Hz and 4Hz) \cite{bhattacharya2005phase}, which overlaps with the frequency range of stimuli tested in our Experiment 1. In contrast, musicians listening to music show synchrony in a more diverse set of neural oscillation frequencies, including delta and gamma (greater than 30Hz) bands \cite{bhattacharya2005phase}. Our model's $D$ parameter could be inversely proportional to the number of neural oscillation frequency bands observed when listening to music compared to resting conditions. Other neuroscientific research regarding the question of how musical expertise affects synchronization indicates that, compared to non-musicians, musicians show enhanced executive function and activation of the supplementary motor area \cite{zuk2014behavioral}, are better able to imitate hand gestures \cite{spilka2010gesture}, and show greater connectivity between premotor and striatal brain areas during beat perception \cite{grahn2009feeling}. Hence, musical expertise affects synchronization. Further, Riley and colleagues \cite{riley2012learning} hypothesize that behaviors involving precision, like perception-action coordination, rely on synergy formation, which is associated with a reduction of the degrees of freedom that a person has to handle cognitively during a task. Through training, synergies associated with synchronization may be greater in musicians than in non-musicians. The $D$ parameter in the SAPPA model could be understood as related to the number of degrees of freedom that a person has to handle during perception-action coordination, since $D$ had a smaller value in the musician SAPPA model compared to the non-musician SAPPA model.

Our model lacks variability in its behavior when synchronizing with an isochronous stimulus. When people tap with an isochronous stimulus, a mean negative asynchrony is observed with a large variability around it \cite{repp2005sensorimotor}. This means that some people may exhibit mean positive asynchronies (e.g. reaction to the stimulus rather than anticipating). The current SAPPA model can only describe the mean anticipatory tendency observed when averaging data across individuals. Variability in the SAPPA model could be achieved with simulations where the $D$ parameter is varied using a gaussian distribution centered around the $D$ values we found for musicians and non-musicians. B\aa\aa th \cite{baaaath2016estimating} used mixed gaussian distributions to simulate the anticipation variability seen in the behavioral experiments similar to those in Repp and Doggett \cite{repp2007tapping}. Using gaussian distributions to add variability to the SAPPA model would also allow models of individual participants to have different parameters. One could test whether the SAPPA model’s $D$ parameter is related to individual’s oscillatory power or years of musical training. The SAPPA model is currently unable to explain anticipatory tendencies or positive asynchronies observed outside the IOI range between 1000ms and 3500ms. Future investigations should test the SAPPA model for anticipation outside the IOI range we investigated in this study. When tapping with IOIs longer than 3500ms, people may show mean positive asynchronies, while mean negative asynchronies are observed when tapping with IOI lengths shorter than 1000ms [5]. We already discussed in the previous paragraphs that the SAPPA model’s non-linearities could explain the positive asynchronies observed for IOIs longer than 3500ms. Although beyond the scope of the current investigations, we did observe that for IOIs shorter than 1000ms, the SAPPA model shows a negative asynchrony, consistent with behavioral data.

\subsection{Experiment 2: Interpersonal synchronization during alternating \\ paced tapping with or without auditory feedback}

In the solo and duet conditions, our musician SAPPA model showed less anticipation when musicians tap every other metronome beat hearing their own actions compared to when only hearing a metronome. When the SAPPA model synchronizes with its own non-delayed activity in addition to the external sinusoid, the input $F = \frac{\exp(i2\pi f_s t)+Az}{|\exp(i2\pi f_s t)+Az|}$ is the sum (as shown by Eq \eqref{eq:2.7} in model definition in the methods section). The smaller anticipation when as opposed to when $F = \exp(i2\pi f_s t)$ (i.e. only the external sinusoid) is caused by the instantaneous phase shift that $F$ suffers when $z$ gets subtracted from the external sinusoid $\exp(i2\pi f_s t)$, causing $F$ and $z$ to be more phase-aligned. In contrast, when only synchronizing with an external sinusoid, the delayed recurrent feedback (see Eq \eqref{eq:2.5} in model definition in the methods section) linearly adds with $F = \exp(i2\pi f_s t)$, resulting in a delayed phase shift, causing $F$ and $z$ to be less phase-aligned.

We simulated anticipation using the musician and the non-musician SAPPA models. As shown in Fig \ref{f2_3}, the musician SAPPA model was able to show smaller anticipation in the feedback-on compared to the feedback-off condition in both solo and duet tasks. While the task by Nowicki and colleagues \cite{nowicki2013mutual} has not yet been tested with non-musicians, our simulations predict that the non-musician anticipation will generally be larger than the musician anticipation observed across tasks and conditions. We speculate that this prediction is highly likely, because it has been previously reported that non-musicians’ asynchronies are larger than musicians’ asynchronies \cite{repp2007tapping}. By collecting data from non-musicians carrying out this task one could test the validity of our model’s predicted behavior.

Currently, the SAPPA model does not include modules that represent variabilities over time across individuals in a sample population. This means that it is limited in its ability to explain long-range temporal dynamics. Hence, anticipatory dynamics are limited to a local time scale. Incorporating noise in our model is beyond the scope of the work we present here, because the nature of noise as well as its sources and mechanisms remain largely unknown. As previously discussed, B\aa\aa th \cite{baaaath2016estimating} has modeled the noise associated with anticipation in the simplest metronome tapping task using a mixed gaussian distribution. To our knowledge, no studies have demonstrated how neural mechanisms and their intrinsic noise might be related to synchronization phenomena and anticipation. Variations in the environment and task requirements are also likely to add different types of noise that influence the overall system behavior. To effectively simulate different noise sources, it would be necessary to first study the distributions and mechanisms underlying variability in human adaptive behavior, using a large corpus of empirical data. Additionally, when quantifying how noise affects synchronization at local and global time scales, researchers should carefully consider that asynchronies tend to fluctuate on long-range time scales, which can affect the interpretation of behavioral data analysis and computational models \cite{torre2008distinct}.

Because the two musicians in the duet task tapped every other beat in alternation, the resulting interpersonal synchronization was anti-phase. Recent studies have used the HKB model \cite{haken1985theoretical} (further discussed in the general discussion subsection below) to explain anti-phase interpersonal synchronization \cite{duarte2012interpersonal, dumas2014human, kelso2009virtual}. The SAPPA model and the HKB models are both able to explain anti-phase synchronization and anticipation at different phase relationships. Future investigations could analyze similarities and differences between these models when they synchronize with external periodic stimuli.

\subsection{Experiment 3: Interpersonal synchronization during rhythm-\\ clapping alternation in the presence of transmission latencies}

In the simulations in Experiment 3, two types of delay affected synchronization dynamics between coupled dynamical systems. The first one existed in the SAPPA model, as self-referential recurrent feedback delay in Eq \eqref{eq:2.5}, which resulted in anticipation (see model definition and parameter analysis in the methods section). The second one, TL, was introduced as the external informational delay in the communication between two oscillators, which is extrinsic to Eq \eqref{eq:2.5}. Our simulations integrate these two separate delays to explain how internal and external feedback in our model affect synchronization. Experiment 1 results showed that the first delay resulted in the anticipation, while the second delay can cancel the anticipatory behavior, resulting in disrupted synchronization. Our simulation results show that TLs similarly affected the synchronization of the musician and non-musician SAPPA models (Fig \ref{f2_4}C and \ref{f2_4}D). This suggests that TLs affect synchronization independent of musical expertise. Future experiments can test our model’s prediction of non-musician behavior by collecting data when non-musicians carry out the task introduced by Chafe and colleagues \cite{chafe2010effect}.

In Experiment 3, pairs of oscillators ($f = 1.5$Hz) stimulated each other in an alternating fashion every cycle. There was no external sinusoid stimulating the oscillators in this experiment (see setup, procedures and measurements in the methods section for a complete description). One caveat to our current model, is the fact that it cannot explain the positive values observed in existing human behavioral data (Fig \ref{f2_4}C and \ref{f2_4}D). In the experiment by Chafe and colleagues \cite{chafe2010effect}, pairs of participants synchronized in the absence of the natural TL over air (due to wearing headphones), and in the presence of artificial TLs of fixed length. In the real world, synchronizing individuals have to always cope, at least, with the TL associated with sound travelling in air. We speculate that humans cope with such TLs by overestimating the frequency of their actions (i.e. a faster frequency), to make their actions reach the other participant on time. In the experiment by Chafe and colleagues \cite{chafe2010effect}, when neither air-travelling sound TL nor artificial TL existed, participants actions anticipated each other. The anticipation, indicated by positive values in the behavioral data in Fig \ref{f2_4}B, is likely caused by musicians’ expectation of the TL implicit in the transmission of sound in air, which is effectively removed when the TL is very small (<10ms). Our model is naive to the TL of sound travelling in air, and thus it was not affected in the same way by TLs smaller than 10ms. In the SAPPA model, the relationship between TL and the natural frequency of the oscillator can only be achieved by adding a bias that makes the SAPPA model complete its cycles before the driving stimulus (another SAPPA model in Experiment 3) completes a cycle. The $f$ term, which is a constant dictating the natural frequency of the SAPPA model’s oscillatory behavior in units of hertz (see Eq \eqref{eq:2.5} in the model definition in the methods section), can be altered to explain the positive values in Fig \ref{f2_4}B:  

\begin{equation}
\hat{f} = f + \triangle \label{eq:2.1}
\end{equation}

Here, in Eq \eqref{eq:2.1}, a positive offset $\triangle$ (positive real value bias) can be added to compensate for the effect of the transmission of sound through air. This $\triangle$ term would make the SAPPA model’s frequency term slightly faster compared to the external stimulus, resulting in the positive values observed in Fig \ref{f2_4}B for TLs smaller than 10ms. Fig \ref{f2_5} shows our model's resulting behavior when $f$ is modified as described in Eq \eqref{eq:2.1}.

\begin{figure}
    \centering
    \includegraphics[width=1.0\textwidth]{figures/fig2_5.png}
    \caption[The effect of modified $f$ (Eq \eqref{eq:2.1}) on our model's simulation in Experiment 3 where pairs of musicians alternatively clap a rhythm in the presence of transmission latencies]{\textbf{The effect of modified $f$ (Eq \eqref{eq:2.1}) on our model's simulation in Experiment 3 where pairs of musicians alternatively clap a rhythm in the presence of transmission latencies.} The lead and lag between pairs of musician (A) and non-musician (B) models, measured as the percentage of a 90 bpm beat as a function of TLs, with $f$ modified as described in Eq \eqref{eq:2.1}. The linear regression on the behavioral data from Fig \ref{f2_4}B is shown for comparison purposes.} 
    \label{f2_5}
\end{figure}

The behavior in Fig \ref{f2_5} is the result of the frequency detuning introduced in Eq \eqref{eq:2.1}. Previous studies show that frequency detuning and delayed coupling between synchronizing HKB systems (see the General discussion below for more information about the HKB model) result in anti-phase solutions that are less stable than in-phase activity \cite{slowinski2016effects, avitabile2016beyond}. The behavioral data by Chafe and colleagues\cite{chafe2010effect} supports these observations, since musicians performed the same clapping rhythm in-phase, in an alternating fashion. As the transmission delay grew, in-phase synchronization became harder for pairs of synchronizing musicians. To cope with different TL lengths, musicians had to re-adjust the frequency of their actions every cycle to continue in-phase, but very long TLs made in-phase synchronization impossible.

Humans may overestimate the frequency of their actions when synchronizing with each other to cope with sound air travel TLs. The SAPPA model does not overestimate its frequency, and in the lack of the TL associated with sound air travel, one must manually detune the SAPPA model’s f term to result in overestimation. Once this is done the two coupled SAPPA models successfully express the effects of external and internal delays and their interactions in a succinct and integrative manner. Our model can be useful to predict stability of in-phase and anti-phase behavior. Moreover, the SAPPA model could predict how synchronization phenomena like anticipation could be affected by technologies involving TLs, like the ones used for internet-based music performance.

\subsection{General discussion}

The SAPPA model reproduced anticipatory timing during synchronization at different frequencies, for different rhythms, and in tasks of differing complexity. In Experiment 1 we simulated anticipation for isochronous rhythms at a wide range of frequencies, showing the larger negative asynchrony observed in non-musicians compared to musicians (Fig \ref{f2_2}A and \ref{f2_2}B). The only difference between the musician and the non-musician model was the $D$ parameter, which controls the amplitude of the delayed recurrent feedback and was larger in the non-musician model. In terms of neural underpinnings, the $D$ parameter could be inversely proportional to the number of different neural oscillation frequency bands observed when musicians and non-musicians listen to music, compared to resting conditions \cite{bhattacharya2005phase}. Additionally, in Experiment 1 we used our model to predict how the asynchrony would change in an experimental condition where musicians and non-musicians listen and tap with a metronome without hearing their own taps. The model predicts that the asynchrony would grow for both non-musicians and musicians, but the non-musicians' asynchrony would be larger (Fig \ref{f2_2}C). Using the musician model and parameters used in Experiment 1, in Experiment 2 we simulated musicians' solo and duet synchronization with a metronome in feedback-on and feedback-off conditions. We reproduced behavioral data showing that musicians' asynchronies are larger in the feedback-off condition compared to the feedback-on condition in both solo and duet synchronization settings (Fig \ref{f2_3}A, \ref{f2_3}B, \ref{f2_3}D and \ref{f2_3}E). The smaller asynchrony is caused by a phase shift in the stimulus $F$ when $z$ gets subtracted from the external sinusoid $\exp(i2\pi f_s t)$ (see model definition in the methods section). Furthermore, using the non-musician model from Experiment 1 we predicted how non-musicians would perform in this task, projecting overall larger asynchronies compared to musicians, as well as larger asynchronies in the feedback-off condition compared to the feedback-on condition within the non-musician group (Fig \ref{f2_3}C and \ref{f2_3}F). Finally, using the same model and parameters from Experiments 1 and 2, we demonstrated that the model synchronizes like pairs of musicians do in the presence of a fixed TL and the absence of a metronome stimulus (Figs \ref{f2_4}B, \ref{f2_4}C and \ref{f2_5}A). We explained that the total absence of TL causes humans to speed up their actions, allowing events to be properly timed when sound air travel is considered. Also, using the non-musician model we predicted how pairs of non-musicians would be affected by a fixed TL when synchronizing with each other (Figs \ref{f2_4}D and \ref{f2_5}B). Our experiments use a musician and non-musician model to successfully reproduce anticipatory timing for different rhythmic stimuli and different tasks, while also making predictions about what not-yet-existing behavioral data in musicians and non-musicians would look like. These simulations reproduce timing characteristics previously observed in behavioral studies, and demonstrate that complex tasks can be performed by the same model by only modifying the stimuli and their frequency.

In the absence of time delays, oscillatory synchronization (e.g., Eq \eqref{eq:2.4}) offers only one limited set of explanations for anticipation: the frequency of the oscillation is faster than the frequency of the stimulus \cite{kim2015signal}, underestimation of stimulus period \cite{flach2005transition}, and frequency detuning \cite{kelso1990action}; $f > f_s$. As we see it from a dynamical systems point of view, there are two problems with this explanation. The first is that it is an explanation that requires an explanation: What causes the underestimation of tempo? Secondly, time delays are ubiquitous in the brain and nervous system, thus “adding” time delays to a neural model is not really an adding anything, it is only being more realistic. Neural conduction delays have been conclusively established and measured, and recurrent feedback is equally well-established. Finally, delayed recurrent feedback causes an oscillator with frequency $f$ to oscillate at a frequency faster than $f$. Delayed recurrent feedback causes frequency detuning, thus we could say it causes a perceptual underestimation of IOI. Moreover, oscillatory synchronization has intrinsic frequency detuning features (see Large \cite{large2008resonating} for a theoretical overview of frequency detuning between oscillators).

A large body of literature has studied the dynamics of coordination in a variety of contexts (see \cite{repp2005sensorimotor, repp2013sensorimotor} for reviews). Currently, there are two main views about the neural mechanisms that underlie anticipatory timing. Some researchers have proposed that anticipation results from combining somatosensory and auditory modalities because axonal distances between the hand, the ear, and the brain differ \cite{aschersleben1995synchronizing, prinz1992don}. Axonal delays are fixed and the combination of somatosensory and auditory input is likely carried out by assimilation areas of the brain in a bottom-up fashion. While the combination of different sensory modalities can explain the negative phase relationship between human taps and a metronome in principle \cite{aschersleben2002temporal, bialunska2011increasing}, this theory fails to explain many of the key findings, including the fact that anticipation increases with longer metronome periods in the IOI range between 1000ms and 3500ms (see the review of the tapping literature by Repp \cite{repp2005sensorimotor}). The length of time that it takes for the brain to incorporate multiple sensory modalities is likely fixed, so it can only affect the phase relationship between action and stimulus by a constant value, predicting a constant anticipation across different tempi, which is not consistent with behavioral data.

Other researchers explain that anticipation, in general, is the result of neural computations carried out in progressive stages that result in representations \cite{clark2008brain}. These representations can be about past, present, and future states of the external world. Hence, this view would explain that anticipatory timing emerges from a system’s need to fulfill its representations about the likely future state of the external world \cite{pezzulo2008coordinating}. What makes these two views similar is the fact that they see anticipation as the result of a series of staged neural computations that give rise to a representation that is either misaligned with the external stimulus or fulfilled prematurely.

In contrast to these two theories, our model, inspired by the strong anticipation hypothesis, offers a different explanation based on a dynamical systems approach where the properties of the model and its interaction with the external stimulus and surrounding active systems are mathematically described as constrained by universal physical laws \cite{stepp2010strong}. Stephen and Dixon \cite{stephen2011strong} have described that strong anticipation can happen at local and global temporal scales, where local strong anticipation occurs between systems continuously coupled and global strong anticipation is more complex, involving multi-scale interactions (see \cite{mahmoodi2020dynamical, roume2018windowed} for a thorough discussion of global vs local strong anticipation). The SAPPA model is a clear example of local strong anticipation. All of these observations make the SAPPA model a parsimonious and simple one, whose behavior is the result of interactions with external stimuli, instead of internal representations.

It is noteworthy that other researchers have already built models that could explain and predict human rhythmic coordination behavior by using general physical principles. An important one is the HKB model, which uses a simple equation to explain the possible phase relationships between two coupled oscillatory systems in many contexts, including human intra- and inter-personal synchronization \cite{slowinski2016effects}. Because of its focus on relative phase \cite{calvin2010perspectives}, the HKB model is flexible, parsimonious, and a powerful tool to explain and predict periodic human motor behavior \cite{schmidt1990phase}. By changing two parameters in a single equation, the HKB model is able to explain transitions between anti-phase (180$^\circ$) and in-phase (0$^\circ$) coordination when one person taps both index fingers together or two people swing their feet in tandem at different frequencies \cite{schmidt1990phase}. Its full potential has not yet been exploited, since recent bifurcation analyses revealed that the HKB model comprises previously unreported dynamical regimes that could explain an even wider range of human synchronizing behaviors beyond in-phase or anti-phase regimes \cite{avitabile2016beyond}, including squash \cite{mcgarry2006identifying} or butterfly stroke swimming \cite{ehrlacher2003sports}. Although the HKB model has been widely used to explain synchronization behavior, it lacks an explanation of the neurophysiological and biophysical principles that drive synchronization \cite{peper2004explanatory}. Some exceptions exist though, since studies have attempted to make the HKB model a more biologically plausible one by explaining how neural connectivity delays affect its behavior \cite{banerjee2007neural, slowinski2016effects}. While these results are still focused on in-phase and anti-phase phenomena, the added delay allows the HKB model to make predictions about split-brain patients lacking certain modalities of delayed neural cross talk \cite{banerjee2007neural} or factors that favor in-phase vs anti-phase stability \cite{slowinski2016effects}.

Recently a few more models have been proposed that specifically attempt to account for anticipatory synchronization. The first one is the ADaptation and Anticipation Model (ADAM) \cite{van2013adaptation}. ADAM uses phase and period correction to carry out adaptive and anticipatory synchronization with a periodic stimulus that may contain tempo changes \cite{van2013adaptation}. The second, more recent model is proposed by Bose, Byrne and Rinzel \cite{bose2019neuromechanistic} proposed as a neuromechanistic model of musical rhythm that, similar to ADAM, corrects its phase and period to find the beat in a periodic stimulus. This neuromechanistic model counts 40-Hz gamma-band oscillatory cycles to quantify how well the model’s beat generator aligns with an external stimulus. Beat synchronization is achieved due to the beat generator’s plasticity, which is informed by the gamma-rhythm count. Both ADAM and the more recent neuromechanistic model compute information from an external stimulus using error correction and extrapolation, and therefore are examples of weak anticipation models. Two other earlier models proposed mechanisms that explained synchronization behaviors. One of them was described by Caceres \cite{caceres2013synchronization}, which used an oscillator described by Large and Kolen \cite{large1994resonance} for rhythm tracking and generation. Caceres compared his model’s performance with a memoryless model by Gurevich and colleagues \cite{gurevish2004simulation}, providing evidence that oscillators are better at anticipating musical rhythms than memoryless methods \cite{caceres2013synchronization}. Mates and colleagues proposed that sensorimotor synchronization is influenced by the maximal capacity of temporal integration, which was estimated to be around 3 seconds \cite{mates1994temporal}. This explains why stimuli with IOIs of length greater than maximal capacity of temporal integration cause greater incidence of reactive responses (i.e. after the stimulus onset) and less anticipation compared to stimuli with shorter IOIs.

The SAPPA model is a strong anticipation model because it computes its current state from its physical properties, not through inference \cite{dubois2001incursive,stepp2010strong}. Together with the strong anticipation hypothesis and oscillator modeling, our model’s architecture and internal feedback delays could shed light on physical principles of neural mechanisms that underlie anticipation in human behavior. However, it would be useful to further explore these existing models possibly combined with our approach, for scenarios when actions and intentions require substantial temporal deviations from simple integer-ratio based temporal organizations.

Our model is inspired by previous neuroscientific studies and theories explaining how periodic signals like music are processed in the brain. The canonical model of oscillation incorporated in our work has previously been used to explain how people perceive the beat in complicated rhythms \cite{large2015neural} and how the brain entrains to simple and complex auditory rhythms signals \cite{large2015neural, tal2017neural}. Hence, the SAPPA model's Hopf oscillator has been previously used to theorize about brain mechanisms, specifically concerned with the auditory system, rhythm perception, and neural oscillations \cite{large2015neural, tal2017neural}. In our model's architecture, there are specific components inspired by neuroanatomy and functional units in the human brain involved in synchronization behavior and anticipation. The SAPPA model encodes stimuli in a way that is analogous to what occurs in the auditory cortex in conjunction with timing processing carried out in the basal nuclei and the cerebellum. Additionally, the SAPPA model simulates the looped communication between basal nuclei, sensory areas, and the motor cortex and peripheral systems, all of which are fundamental for synchronization to occur \cite{aschersleben1995synchronizing, prinz1992don}. It is important to note that our model works at the level of the oscillatory neural populations, not at the level of single neuron activity. Evidence shows that single neuron dynamics give rise to oscillatory activity at the level of neural populations \cite{byrne2017mean}. Regarding the timing processing, neuromagnetic data by Fujioka and colleagues \cite{fujioka2012internalized} have shown that listening to an isochronous beat causes periodic amplitude modulation of beta-band (around 20Hz) oscillatory brain activity in the motor and auditory cortices. Furthermore, when beats are accentuated into stereotypical metric patterns, like waltz rhythms, the beta-band predicts the beginning of groupings \cite{fujioka2015beta}. Findings show that beta-band oscillations are not only associated with sensorimotor functions \cite{pfurtscheller1981central, salmelin1995functional} but also the anticipation of event timing \cite{arnal2015delta, breska2016synchronizing}. Using oscillators to explain neural activity is a low-dimensional approximation of the cortical dynamics observed during synchronization. However, as discussed at the beginning, these high-level representations may be far from microscopic information transactions between neuronal populations.

The model we presented in this paper is general, flexible and parsimonious. As long as the behavior and external factors follow oscillatory patterns, the SAPPA model could simulate the outcome. It uses a canonical model of oscillatory dynamics and shows anticipatory behavior following the general principles of the strong anticipation theory. This makes our model a tool to simplify our understanding of how delayed communication within the human sensorimotor system results in synchronization.

Notably, we used the same model across three different experiments that simulated three different behavioral studies published by different groups at different times. These three behavioral studies employed a variety of tasks and measures, yet, our model was able to capture the behavioral patterns and effects observed across all studies. This demonstrates the versatility and generalizability of our model’s architecture. Our experiments focused on simulating tasks in the context of music because we wanted to test our model in ecologically plausible conditions. The results we presented show that our model can explain human musical behavior, which makes it a tool for the prediction of ecologically-valid anticipation in experiments beyond the ones we discussed here. Our work supports the strong anticipation theory, making new predictions about human behaviors that have not been tested yet. Moreover, our model could aid technologies that assist synchronized action-making in teletherapy with TLs. A better theoretical understanding about how TLs and anticipation interact could lead to a system that dynamically calibrates TLs to help synchronization between humans interacting over the internet. Additionally, knowing how an individual’s neural delays affect synchronization could serve as a biomarker for rehabilitation in personalized medicine \cite{slowinski2016dynamic, slowinski2017unravelling}. Finally, our model could be used to improve telecommunications for synchronized action and have implications in other fields like network music performance and robotics.

\section{Methods}

\subsection{Model definition}

\subsubsection{Theoretical background}

Negative phase relationships between two systems, such as anticipation, have been observed in the behaviors of coupled dynamical systems like external-cavity diode lasers \cite{ikeda1980optical} and FitzHugh–Nagumo systems \cite{toral2003characterization}. The following equations by Voss \cite{voss2000anticipating, boccaletti2001space} and Ciszak et al. \cite{ciszak2004dynamical} express the general framework of an anticipatory dynamical system with delayed feedback: 

\begin{equation}
\dot{x}=h(x) \label{eq:2.2}
\end{equation}

\begin{equation}
\dot{y}=g(y)+h(x-y_\tau) \label{eq:2.3}
\end{equation}

Eq \eqref{eq:2.2} describes the stimulus $x$, which has dynamics defined by the function $h$. Eq \eqref{eq:2.3} describes a system y with dynamics defined by the function $g$. In Eq \eqref{eq:2.3}, the subscript $\tau$ indicates delayed behavior of the system $y$. This $y_\tau$ term is referred to as delay-coupling \cite{boccaletti2001space, stepp2015muddle}. A negative phase relationship between $y$ and $x$ is observed when $y$ receives $x$ and its own delayed activity $y_\tau$ as inputs, as described in Eq \eqref{eq:2.3}. Importantly, if $y_\tau$ is removed from these equations, the anticipatory behavior of $y$ in relationship to $x$ is no longer observed \cite{stepp2015muddle}.

Because the SAPPA model simulates periodic synchronization, we used a canonical Hopf oscillator model that allows for synchronization with periodic external stimuli \cite{large2010canonical}. In contrast to the original model described by Large and colleagues \cite{large2010canonical}, the SAPPA model does not consist of a network of oscillators, but a single oscillator shown in Eq \eqref{eq:2.4}. 

\begin{equation}
\frac{1}{f}\dot{z} = z\bigg(\alpha + i2\pi + \beta_1|z|^2 + \frac{\epsilon\beta_2|z|^4}{1-\epsilon|z|^2}\bigg) + F \label{eq:2.4}
\end{equation}

In Eq \eqref{eq:2.4}, $z$ is the state of the oscillator, and $\alpha$, $\beta_1$ and $\beta_2$ are parameters that control the dynamics of the oscillator while $f$ determines the frequency of oscillation. $\epsilon$ is a parameter that controls the degree of higher-order nonlinear activity in the oscillator and $F$ is a stimulus. These dynamics allow a single oscillator to show stable oscillatory activity, even after it is no longer being stimulated, hence showing "memory" \cite{kim2015signal}. Finally, similar to other strong anticipation models, this model can explain behaviors observed in non-biological systems like Wilson-Cowan networks \cite{large2010canonical}, musical pitch recognition \cite{large2010dynamical}, and beat tracking \cite{large2015neural}.

\subsubsection{The SAPPA model}

Eq \eqref{eq:2.5} shows our model, which is the Hopf oscillator in Eq \eqref{eq:2.4} with $\beta_1 = \beta$, $\beta_2=0$, $\epsilon=0$, and an additional delayed feedback term. 

\begin{equation}
\frac{1}{f}\dot{z} = z(\alpha + i2\pi + \beta_1|z|^2) + F - \frac{D}{f}z(t-\tau) \label{eq:2.5}
\end{equation}

Eq \eqref{eq:2.5} in all our simulations has $\alpha=1$ and $\beta=-1$ to achieve limit cycle behavior with periodic, unit-magnitude stable and perpetual activity when $F = 0$ and $D = 0$. The oscillator has a fixed frequency determined by the parameter $f$ (in Hz). The computations by this oscillator are neuroscientifically inspired. It receives an input $F$, thus encoding and perceiving an external stimulus \cite{large2015neural}. It also receives its own delayed activity with amplitude $D$ and a delay of $\tau$ seconds, simulating the delayed communication in the nervous system between basal-ganglia, cerebellum, premotor cortices, motor cortices, and peripheral muscles at extremities and effectors (e.g., fingers), which are inherent in the perception-action cycles \cite{aschersleben1995synchronizing, prinz1992don}. We expect the delayed $z$ feedback to result in a negative phase relationship between $z$ and an external stimulus. In the SAPPA model, summarized by Eq \eqref{eq:2.5}, we vary the amplitude of the $D$ parameter, while $\alpha$ and $\beta$ always have values of $1$ and $-1$, respectively. The value of the variable $f$ is selected to match the frequency $f_s$ of a periodic external stimulus ($f = f_s$). Because $z$ is always unit amplitude, the effect of the delayed recurrent feedback is reduced as $D$ shrinks.

The input $F$ is a sinusoid with a constant frequency $f_s$. Further, if the SAPPA model is to consider its own behavioral outcome as input (e.g., hearing and feeling one’s own tapping feedback), the $z$ activity may be added to the input. Hence, the input $F$ may be: 

\begin{equation}
F = \exp(i2\pi f_s t) \label{eq:2.6}
\end{equation}

\begin{equation}
F = \frac{\exp(i2\pi f_s t)+Az}{|\exp(i2\pi f_s t)+Az|} \label{eq:2.7}
\end{equation}

\begin{equation}
F = Az \label{eq:2.8}
\end{equation}

In Eqs \eqref{eq:2.6} and \eqref{eq:2.7} $f_s$ is the constant frequency of the external sinusoid. Unless otherwise noted, $f_s$ and $f$ are the same value ($f_s = f$). Eq \eqref{eq:2.7} normalizes the linear combination of the external sinusoid $\exp(i2\pi f_s t)$ and $Az$, consistent with behavioral observations of stimulus encoding in synchronization tasks, where changing the amplitude of the stimulus does not affect synchronization \cite{repp2008metrical}.

\subsection{Parameter analysis}

Eq \eqref{eq:2.5} describes $z$'s derivative over time as the linear combination of three terms: the oscillator's current state $z$, a stimulus $F$ (see Eqs \eqref{eq:2.6}, \eqref{eq:2.7}, and \eqref{eq:2.8} for the different possibilities), and the delayed recurrent feedback $z(t - \tau)$. Previous studies have investigated how $z$ synchronizes with a periodic stimulus \cite{kim2015signal, large2008resonating}. For this reason, our parameter analysis focuses on how the time length $\tau$ and amplitude of the delayed recurrent feedback $z(t - \tau)$ affect the anticipation of the SAPPA model.

Throughout this paper, we fixed the parameters $\alpha = 1$ and $\beta = -1$ so that $z$ shows unit-magnitude behavior when $F = 0$ and $D = 0$. When only $D = 0$, $z$ and the external stimulus $F$ have the same frequency and phase, so anticipation is absent (see Fig \ref{f2_6}), independent of which version of $F$ is used (remember Eqs \eqref{eq:2.6}, \eqref{eq:2.7} or \eqref{eq:2.8} are the three possible version of $F$). The other parameter in Eq \eqref{eq:2.5} that can be varied is $f$, but since it is only a scale factor that affects the cycling rate of $z$, its effects will be studied later in Experiment 1. In cases when $F = \exp(i2\pi f_s t)−Az$, the magnitude of $A$ is another term that can be varied. Hence, the only other terms in Eq \eqref{eq:2.4} that can be varied are $D$, and $\tau$.

\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{figures/fig2_6.png}
    \caption[Analysis of the effect that different parameters in the SAPPA model have on its anticipation tendency]{\textbf{Analysis of the effect that different parameters in the SAPPA model have on its anticipation tendency.} (A) Illustration of what the asynchrony between the SAPPA model and the external sinusoidal stimulus can look like, and how it’s measured. (B-F) Analysis of the anticipation as a function of $D$ and $\tau$ in Eq \eqref{eq:2.5}, and $A$ in Eq \eqref{eq:2.7}: (B) $A = 0$, (C) $A = -1.0$, (D) $A = -0.5$, (E) $A = 0.5$, (F) $A = 1$. In these analyses (B-F) the parameter $f = 1$. The numbers in each cell indicate the anticipation (in ms) observed when the SAPPA model synchronized with the external sinusoidal stimulus. A black cell indicates that the SAPPA model did not synchronize with the external sinusoidal stimulus and hence the asynchronies could not be computed. In the analyses (B-F), the asynchrony quickly moves away from zero as $0 < \tau < 0.5$, especially when $D = 1$. Additionally, we explored how different initial conditions affect the model’s asynchrony and discontinuities, described in the Supplementary Fig \ref{fS_2} which contains the bifurcation diagram for the SAPPA model when $D = 1$.} 
    \label{f2_6}
\end{figure}

For this parameter analysis we fixed $f = 1$ to ignore its scaling effect. We analyzed the effect of $D$ in the range of values between $0$ and $1$, which is the dynamic range of $z$ when $F = 0$ and $D = 0$, and also the dynamic range of $F$, considering all three Eqs \eqref{eq:2.6}, \eqref{eq:2.7} or \eqref{eq:2.8}. For $\tau$, we analyzed its effect in the range of values from $0$ seconds to $0.5$ seconds, because the period associated with $f = 1$ is one second and a processing delay with duration closer to the stimulus period length would not make sense during synchronization. To observe the effect of $A$, we also tested $A = -1$, $-0.5$, $0$, $0.5$, and $1$, which are all values within the dynamic range of $z$ (when $F = 0$ and $D = 0$) and $F$ (note that when $A = 0$, Eqs \eqref{eq:2.6} and \eqref{eq:2.7} are the same).

We evaluated the anticipation for all possible combinations of $D$, $\tau$, and $A$ parameters. At the beginning of each simulation, the phase of $z$ was initialized to zero. During the first few sinusoidal cycles, the SAPPA model phase-locked with the external sinusoid in order to achieve a state of stable synchronization. From a dynamical systems perspective, 'synchronization' refers to the relationship between one system's actions as closely adhering to another system's actions \cite{pecora1990synchronization}. Stable synchronization was observed when the phase difference between the stimulus sinusoid and the model's oscillators reached a constant value over time. This stable mode is known in dynamical systems theory as a steady state. Fig \ref{f2_6}D shows how we calculated the asynchrony between our model's oscillator $z$ and the stimulus sinusoid. After a simulation reached a steady state, we first found the timepoints of the peaks for the real part of the $z$ oscillator and the stimulus sinusoid. Then, we subtracted each stimulus sinusoid's peak time from the nearest $z$ oscillator's peak time. The average peak time difference between the stimulus sinusoid and the $z$ oscillator over time was considered the model's mean asynchrony. Our results are shown in Fig \ref{f2_6}. To visualize the results of these simulations, we plotted these results in matrix form, with the x axis indicating the value of $\tau$, the y axis the value of $D$, and the color in the matrix cells indicating the asynchrony. We obtained five matrices, one for each of the five values of $A$ tested.

\subsection{Experiment 1: Individual tapping in synchronization with an \\ isochronous stimulus}

\subsubsection{Behavioral data for simulation}

In the task by Repp and Doggett \cite{repp2007tapping}, musicians and non-musicians tapped in synchrony with an isochronous metronome in a frequency range between 1Hz and 0.29Hz (corresponding to period durations between 1000ms and 3500ms). They found that the anticipatory tendency increased as a function of metronome period length. Fig \ref{f2_2}B shows our models' anticipation, as well as the linear regression on the behavioral data (Fig 1A in Repp and Doggett \cite{repp2007tapping}). We computed the linear regression lines as the best fit on the anticipation values for musicians and non-musicians.

\subsubsection{Setup, procedures and measurements}

Fig \ref{f2_1}A shows the human task and a graphical explanation of our simulation setup that contain the SAPPA model and the input stimulus. In these simulations, the input $F$ was $F = \exp(i2\pi f_s t) + Az$ because our model "listened" to itself (i.e. received its own instantaneous activity as input). In the parameter analysis section above, we studied the model's behavior for a frequency of 1Hz. In the human task simulated in this experiment, humans tapped with metronomes of period lengths between 3500ms (approx. 0.2857Hz) and 1000ms (1Hz). Hence, we repeated the parameter analysis with $f = 0.285$ (Fig \ref{f2_7}) to observe the model's behavior at the other end of the spectrum of stimulus frequencies corresponding to this human task. Remember that $f = f_s$. always, unless otherwise noted.

\begin{figure}
    \centering
    \includegraphics[width=1.0\textwidth]{figures/fig2_7.png}
    \caption[Analysis of the effect that different frequencies in the SAPPA model have on its anticipation]{\textbf{Analysis of the effect that different frequencies in the SAPPA model have on its anticipation.} (A) Analysis of the anticipation as a function of $D$ and $\tau$ in Eq \eqref{eq:2.5} when $A = -0.5$ and $f = 1$. (B) Analysis of the anticipation as a function of $D$ and $\tau$ in Eq \eqref{eq:2.5} when $A = -0.5$ and $f = 0.2857$. The numbers in the cells indicate the anticipation (in ms) observed when the SAPPA model synchronized with the external sinusoidal stimulus.}
    \label{f2_7}
\end{figure}

\subsubsection{Model optimization}

To identify the model matching the musician and the non-musician anticipation curves, we identified the set of parameters $D$, $\tau$, and $A$ that resulted in the best fit between the simulated model's anticipation and the slopes of the linear regression for the behavioral data (Fig \ref{f2_2}B). We found that $A = -0.5$, $\tau = 0.222$ seconds, and $D = 0.05$ for the musician SAPPA model and $D = 0.36$ for the non-musician SAPPA model.

For all experiments that we conducted $A = -0.5$. This implies that our model's behavior is half of the magnitude of the external stimulus $\exp(i2\pi f_s t)$, which always has a magnitude of $1$, meaning that the model is forced more strongly by the external sinusoidal stimulus than by its own activity and that the SAPPA model's phase-locking behavior will be greatly determined by the phase of the external sinusoid \cite{kim2015signal}. In the SAPPA model, all recurrent feedback terms are negative. That is why $A = -0.5$ is negative in Eqs \eqref{eq:2.7} and \eqref{eq:2.8}, like the delayed recurrent feedback term in Eq \eqref{eq:2.5}. In Eq \eqref{eq:2.7}, $z$ affects the encoding of the external stimulus, shifting the phase of $F$ in a negative direction with respect to $\exp(i2\pi f_s t)$. Behaviorally, this means that when the SAPPA model listens to itself in addition to the stimulus $\exp(i2\pi f_s t)$, its actions will be more aligned with $F=\frac{\exp(i2\pi f_s t)+Az}{|\exp(i2\pi f_s t)+Az|}$, thus resulting in reduced anticipation compared to when $F = \exp(i2\pi f_s t)$.

\subsection{Experiment 2: Interpersonal synchronization during alternating \\ paced tapping with or without auditory feedback}

\subsubsection{Behavioral data for simulation}

Among the behavioral data shown by Nowicki and colleagues \cite{nowicki2013mutual}, we were focused on simulating the results from solo and duet tasks. First, in the solo task, musicians tapped every other beat in synchrony with a metronome while hearing their own taps (feedback-on) or only hearing the metronome (feedback-off). This means that musicians' tapping behavior had a subharmonic rate with the stimulus rate (i.e., if the stimulus was presented at 2 Hz, tapping should occur at 1 Hz). As shown in Fig \ref{f2_3}A, anticipation was larger when musicians could not hear their own taps compared to when they did. Second, in the duet task, pairs of musicians alternately tapped with a metronome while hearing their own and the partner's taps (feedback-on) and also while only hearing the metronome (feedback-off). The anticipation was also larger when musicians could not hear each other in the duet task, as shown in Fig \ref{f2_3}D.

\subsubsection{Setup, procedures and measurements}

To simulate the feedback-off condition, we removed the $z$ term in the $F$ input, meaning that the model received input only from the external sinusoid $F = \exp(i2\pi f_s t)$. To simulate the feedback-on condition, we added the model's own $z$ activity to the input $F$ during the second half of every stimulus cycle. Finally, to simulate the duet task, we connected two SAPPA models, both synchronizing with the same external sinusoid. One of the SAPPA models exhibited in-phase synchronization with the external sinusoidal stimulus and the other one synchronized antiphase with respect to the external sinusoidal stimulus. To achieve in-phase and antiphase synchronization, the sinusoidal stimulus had a positive sign (i.e., $F = \exp(i2\pi f_s t)$) and a negative sign (i.e., $F = -\exp(i2\pi f_s t)$), respectively. To simulate the feedback-on condition, the two synchronizing SAPPA models alternately received one of the model's $z$ activity as input during the first half of every stimulus cycle, and the other model's $z$ activity during the second half of every stimulus cycle.

We used the SAPPA model and the parameter set determined for musicians' data in Experiment 1. The simulations of the solo task started with a musician SAPPA model receiving an external sinusoid of frequency $f_s = 1$Hz (similarly, the SAPPA model's $f = 1$Hz). To simulate the solo task with auditory feedback, we observed the model's behavior while its own $z$ activity was added as an additional input during the second half of every stimulus cycle (feedback-on) (see Fig \ref{f2_1}B). This means that, the input alternates between $F = \exp(i2\pi f_s t)$ and $F = \exp(i2\pi f_s t) + Az$ at a frequency twice as fast as the external sinusoid, to follow the task design (e.g., listening to a metronome click, then listening to both a metronome click and own tapping). To simulate the condition without auditory feedback, the external sinusoid was set as the only input throughout the simulation (feedback-off, the input was always $F = \exp(i2\pi f_s t)$). This task design and the corresponding model setup are indicated in Fig \ref{f2_1}B and \ref{f2_1}C, respectively.

To simulate the duet task indicated in Fig \ref{f2_1}D, we paired two models, of which one was referred to as model 1 and the other as model 2. To simulate the duet condition with auditory feedback, the input to model 1 was set to $F = \exp(i2\pi f_s t) \pm Az(k)$, while the input to model 2 was set to $F = -\exp(i2\pi f_s t) \pm Az(k)$, where $|A| = 0.5$ and $k$ alternated between 1 and 2 at a rate twice as fast as the frequency of the external sinusoid, as illustrated in Fig \ref{f2_1}D. As stated earlier, the sign of $\pm Az(k)$ depends on whether $z$ indicates recurrent feedback (positive) or the other model’s activity (negative). To simulate the duet condition without auditory feedback, both models received only the external sinusoid as input ($F$ was always $F = \exp(i2\pi f_s t)$ for model 1 and $F = -\exp(i2\pi f_s t)$ for model 2). Fig \ref{f2_1}E illustrates the simulation of the condition without auditory feedback.

The mean asynchrony between the stimulus and the in-phase synchronizing model was computed in the same manner as Experiment 1, as the difference between the peak of the real part of the models' oscillation and the closest peak timepoints in the real part of the stimulus sinusoid. The mean asynchrony between the stimulus and the anti-phase synchronizing model was computed as the difference between the peak of the real part of the models' oscillation and the closest valley timepoints in the real part of the stimulus sinusoid.

\subsubsection{Model optimization}

The SAPPA models in this experiment were the same musician and non-musician models identified in Experiment 1. Therefore, the model architecture used in this experiment is summarized by Eq \eqref{eq:2.5}. Simulations lasted a total of 20 seconds.

\subsection{Experiment 3: Interpersonal synchronization during rhythm-\\ clapping alternation in the presence of transmission latencies}

\subsubsection{Behavioral data for simulation}

In the behavioral paradigm used by Chafe and colleagues \cite{chafe2010effect}, two musicians in different rooms alternately clapped a staggered and looping rhythmic pattern together for an extended period of time. TLs were introduced between the musicians to simulate transmission delays over the internet when remotely located individuals play music together. Such latencies via an internet connection typically range from 20ms to 100ms \cite{caceres2010jacktrip, caceres2008edge}. The rhythmic task consisted of three claps interspaced with periods of relative length of 1-1-2. The two musicians synchronized and performed the pattern in a staggered manner with a half pattern overlap; the first musician started the first half of the pattern with doing two claps (1-1-) alone, and at the third clap (2-), the starting point of the second half of the pattern, the second musician started the first half of the pattern from the beginning (1-1-). When the second musician started the second half of the pattern (2-), the first musician returned to the first half of the pattern (1-1-) and so on. They repeated this without interruption for about 30 s (see Fig \ref{f2_1}F). Before starting, the first musician heard a metronome for six counts. The metronome was randomly set at a tempo of either 86 bpm (beat-per-minute), 90 bpm, or 94 bpm to avoid habituation effects. The second musician did not hear the metronome and joined after hearing the auditory outcomes of the starter’s actions for the first half of the pattern. Delivery of auditory outcome information between subjects was bidirectionally delayed by a TL. For a given trial, the latency stayed at a constant value. Latencies between 3ms and 78ms were examined. The results show that with TLs longer than 20ms the two musicians decelerated their common tempo. For the latencies shorter than 10ms, they accelerated instead. As shown in Fig \ref{f2_4}B, the asynchrony between the two musicians at every cycle grew as a function of the TL.

\subsubsection{Setup, procedures and measurements}

To simulate this paradigm, we used a pair of musician SAPPA models like the one developed in Experiment 1, and set them up to feed one model's $z$ activity as an input to the other at a given cycle, then alternate this input flow direction (see Fig \ref{f2_1}F). The first model is considered to be the initiator and the other, the joiner. In our simulations, $f$ for both oscillators was set to be a frequency of 1.5Hz, equivalent to 90 bpm. For the sake of simplicity, we did not employ the nearby offsets of this frequency that were used by Chafe and colleagues \cite{chafe2010effect} to mitigate adaptation to a specific tempo by the subjects. The simulations were also carried out with pairs of non-musician SAPPA models. Compared to Experiment 1 and Experiment 2 where an external sinusoid stimulated the SAPPA model, in Experiment 3 pairs of oscillators stimulated each other, and there was no external sinusoid.

A simulation of the original experiment began by allowing the initiator SAPPA model to oscillate for one cycle. During this cycle, the initiator received its own $z$ activity as an input (the input to the initiator was $F = Az(1)$) and sent its $z$ activity as an input to the joiner (the input to the joiner was $F = z(1)$). After receiving an input from the initiator for one cycle, the joining model continued oscillating one cycle getting its own $z$ activity as an input (the input to the joiner was $F = -Az(2)$) and sending its $z$ activity to the initiator (the input to the initiator is then $F = z(2)$). In subsequent cycles, the inputs to the two models continued alternating every cycle which oscillator's $z$ activity was used as the inputs to both oscillators. At a given cycle, whichever model's $z$ activity was used as an input to both models was referred to as the 'active' model, while the other one was referred to as the 'passive' model. This alternation was repeated until the simulation had run for 30 seconds to complete a trial. Trials were carried out in the presence of fixed TLs ranging from 0ms to 78ms between models.

We measured the Lead/Lag relationship between models in the same way as Chafe and colleagues \cite{chafe2010effect}. For each participant clapping in turn, the Lead/Lag relationship with respect to the other participant was measured as: 

\begin{equation}
L = (a(1) - b(1)) + (b(2) - a(2)) \label{eq:2.9}
\end{equation}

Where $a(1)- b(1)$ is the time difference in seconds between the first clap of the participant currently in turn and the last clap of the participant previously in turn. $b(2)–a(2)$ is the time difference between the first clap of the participant clapping after the current participant in turn and the last clap of the current participant in turn. $L$ can be expressed as a fraction of a 90 bpm tempo, as in Chafe and colleagues \cite{chafe2010effect}, using the expression: $- 90 \times L \div (60 \times 1000)$, where $L$ (in ms) gets converted to a fraction of a period corresponding to a 90bpm tempo. If the resulting value was negative, this meant that the passive oscillator lagged behind the active oscillator. A positive value suggests the opposite.

\subsubsection{Model optimization}

The SAPPA model in this experiment was similar to the musician and non-musician models identified in Experiment 1. Therefore, the model architecture used in this experiment is summarized by Eq \eqref{eq:2.5}. Compared to Experiments 1 and 2, in this experiment these was no external sinusoidal stimulus. Instead, pairs of SAPPA models stimulated each other.

\chapter{Hebbian tempo learning with elasticity explains how a musician's spontaneous motor tempo affects periodic synchronization}
\chaptermark{Hebbian learning explains synchronization}

\section{Introduction}


\chapter{Motivations to combine delayed feedback and elastic Hebbian frequency learning}
\chaptermark{Combining delayed feedback \& elastic hebbian learning}

The SAPPA and ASHLE models (described in chapters 2 and 3, respectively) can explain different types of periodic sensorimotor synchronization. While both share the oscillatory dynamics of the equation originally described by Large and colleagues \cite{large2010canonical}, their biopysical mechanisms are very different. The SAPPA model involves delayed feedback to explain the anticipation tendencies observed in humans, while the ASHLE model involves Hebbian learning and elasticity to explain adaptive synchronization affected by the spontaneous motor tempo (SMT). On their own, these models have limitations but a more robust model could be developed by combining all the biophysical mechanisms present in SAPPA and ASHLE.

A major limitation of the SAPPA model is its complete lack of variability. In our experiments with the SAPPA model, we identified two different delayed feedback amplitudes able to simulate the negative mean asynchrony (NMA) that Repp and Doggett \cite{repp2007tapping} observed in groups of musicians and a non-musicians. In our simulations, a single SAPPA model explained all musician results, and another SAPPA model simulated all non-musician results. The only difference between these two models was the amplitude of the delayed feedback term. This makes the SAPPA model parsimonious and able explain the average NMA observed in a group consisting of musicians and another group consisting of non-musician. However, the SAPPA model cannot explain the NMA differences between individuals in a group. Moreover, the frequency term in the SAPPA model is fixed and has to be manually tuned to match a periodic stimulus tempo and simulate the NMA. 

In contrast to SAPPA, the ASHLE model does not suffer from a complete lack of variability in its behavior. ASHLE's frequency term is dynamically learned from a stimulus. Additionally, ASHLE can simulate the SMT at the individual level (not the group level). Moreover, our simulations show that noise can be added to add variability to ASHLE's instantaneous activity. However, the ASHLE model lacks the delayed feedback term present in the SAPPA model and it remains unclear whether the ASHLE model can explain the NMA to the same extent that the SAPPA model does. 

Given the differences between the SAPPA and ASHLE models, a logical next step would be to combine them. If we assume that the ASHLE model optimally captures the variability of human synchronization, the next step could be as simple as adding the SAPPA model's delayed feedback term to the ASHLE model. Doing this would simply add "strong anticipation" (see chapter 2) to the ASHLE model. The resulting model could be named Strong Anticipation in Adaptive Synchronization with Hebbian Learning and Elasticity (SAASHLE). 

In SAASHLE, the elasticity and strong anticipation mechanisms would be redundant when simulating human synchronization with a stimulus slower than an individual's SMT. For the specific case of musicians, this redundancy would be negligible because the delayed feedback amplitude originally found for SAPPA is relatively small. However, in the case of non-musicians, this redundancy would be observed in the model's activity. This is not necessarily a bad thing, because such redundancy exists in the biophysical mechanisms of adaptive synchronization and anticipation observed in non-musicians. Behavioral data supports this observation. In the original publication by Scheurich and colleagues \cite{scheurich2018tapping}, their Figure 5 shows the asynchrony observed across musicians (top) and non musicians (bottom) that performed a melody with a metronome 15\% and 30\% faster slower than the individual’s SMT. While the musician data shows a similar asynchrony for the faster and slower conditions, the non-musicians show larger negative asynchronies for the slower rate conditions than the faster rate conditions \cite{scheurich2018tapping}. ASHLE on its own would not be able to explain this difference between musicians and non-musicians. However, adding SAPPA's strong anticipation to ASHLE will explain the biophysical mechanisms of this difference observed between musicians and non-musicians. SAASHLE will also be able to explain the variability observed in all human data described in Chapter 2 that SAPPA could not explain. 

All these considerations indicate that future research should aim at developing the SAASHLE model that we have described in this chapter. SAASHLE parameters should be optimized to explain the musician and non-musican data collected by Scheurich and colleagues \cite{scheurich2018tapping}. Subsequently, SAASHLE must be validated by its ability to explain the human data in all other simulations described in chapters 2 and 3. 

SAASHLE will be a powerful model able to explain adaptive synchronization via the biophysical mechanisms of delayed axonal delays (simulated with strong anticipation \cite{stepp2010strong}), neural population entrainment to auditory stimulus (simulated with Hebbian learning \cite{righetti2006dynamic}), and the motor constraints that give rise to the SMT (simulated with elasticity \cite{lambert2016adaptive}). SAASHLE will also capture variability within and between subjects, as well as the effects of musician training via the delayed feedback amplitude difference between musicians and non-musicians models. 


\chapter{Next-generation gradient frequency neural networks}
\chaptermark{Next-Generation GrFNNs}

\section{Abstract}
The units in a Gradient frequency neural network (GrFNN) are similar to the units found in deep neural networks, but each GrFNN unit has its own intrinsic temporal dynamics. Because of their rich set of parameters and dynamics, GrFNNs can carry out pattern recognition with or without memory, signal filtering, and Hebbian learning. To optimize GrFNNs parameters, one can carry out a steady-state analysis with a desired amplitude, frequency and phase. This is mathematically straight forward when the GrFNN stimulus is simple, like a pure tone or a song with a very clear rhythmic beat. However, finding GrFNN parameters is more challenging when the stimulus is noisy, like recordings of speech in real-world environments. We propose that the next generation of GrFNNs should be optimized via gradient descent. We present the first implementation of GrFNNs with tensorflow 2, allowing for automatic differentiation of GrFNN parameters and gradient descent optimization. We carry out two experiments with our implementation. The first one demonstrates that gradient descent can be used to find GrFNN parameters for previously known dynamical states, like limit cycles. The second experiment shows how GrFNN parameters can be optimized with a data-driven approach, similar to deep neural networks. Besides gradient descent optimization, this new implementation allows for future research that integrates GrFNNs as building blocks of deep learning algorithms and other machine learning methods.

\section{Introduction}

The work presented in the preceding chapters uses the canonical oscillator introduced by Large and colleagues \cite{large2010canonical} (see the methods section in chapters 2 and 3 for a detailed description of this equation).  

\begin{equation}
\frac{1}{f}\dot{z} = z\bigg(\alpha + i2\pi + \beta_1|z|^2 + \frac{\epsilon\beta_2|z|^4}{1-\epsilon|z|^2}\bigg) + x(t) \label{eq:5.1}
\end{equation}

The SAPPA (described in chapter 2) and ASHLE (described in chapter 3) models only use one such oscillator each, but in the original publication by Large and colleagues, multiple oscillators were connected in a networks where each oscillator had a different natural frequency \cite{large2010canonical}. A network of these oscillators results in a model called Gradient Frequency Neural Network (GrFNN). A GrFNN shows complex behavior and dynamics that emerge from the interactions between oscillators. The oscillators in a GrFNN are derived from a biological model of neural oscillation observed in the brain, and since their original publication, scientific investigations have shown that GrFNNs can explain the synchronization dynamics observed in real human brain data \cite{tal2017neural}. Moreover, GrFNNs have also been used to simulate the tonotopy and mechanics of the cochlea, including its nonlinearities \cite{lerud2019canonical}.

Besides modeling behavioral and neural data, GrFNNs have properties for signal processing of time-series signals\cite{kim2015signal}. GrFNNs are a generalization of gammatone filter banks \cite{large2015learning}, they can identify temporal patterns, and can amplify or filter periodic features in a signal \cite{kim2015signal}. Some patterns of activity that oscillators in a GrFNN can show include spontaneous limit-cycle activity, which is equivalent to a complex sinusoid, and double limit-cycle activity, which is a memory state that reflects the period of a stimulus even after the stimulus ceases \cite{kim2015signal}. The double limit-cycle dynamics of GrFNNs are possible due to its higher-order nonlinearities (see Eq \eqref{eq:5.1} \cite{large2010canonical, kim2015signal}). Additionally, the oscillators in a GrFNN network can be connected via Hebbian plasticity to adaptively change their connection weights \cite{lambert2016adaptive, kim2017dynamical}. 

GrFNNs are not to be confused with the popular kind of neural networks currently used in deep learning algorithms. There exist some similarities but also very clear differences between the computations in a GrFNN and a deep neural network of the feedforward, recurrent, or convolutional type, to name a few. Both are nonlinear, but for different reasons. In a feedforward neural network, each unit computes the linear combination of weights that multiply input features. Then, each unit’s output may be passed through a nonlinear function, like a sigmoid or a rectifier \cite{svozil1997introduction}. Hence, in a feedforward neural network, each unit carries out a linear computation and the unit’s output is nonlinearly transformed by a function. The units of convolutional and recurrent neural networks obtain their nonlinear properties in a similar fashion, although recurrent neural networks involve feedback, which can be nonlinear on its own \cite{le2015tutorial2}. Moreover, feedforward and convolutional networks do not process stimuli throughout timesteps and their units are temporally static. This is different in recurrent neural networks, where each unit processes information from the present and the past \cite{le2015tutorial2}. In contrast, the units of GrFNNs have intrinsic nonlinear and temporal dynamics. Each oscillator, or unit, computes the rate of change of its own activity, which depends on a nonlinear transformation of the oscillator's current state (see Eq \eqref{eq:5.1}) \cite{large2010canonical}. 

The optimization of patterns in a GrFNN and a neural network is also different. The parameters in a feedforward neural network, for example, can be optimized via gradient descent. The output is compared against a ground truth via an objective function (like mean squared error, for example), and the objective function’s derivative is computed with respect to every parameter to update the feedforward neural network’s parameters \cite{le2015tutorial1}. This process to update the model parameters is iterated until the objective function error is maximally reduced. In a GrFNN the parameters $\alpha$, $\beta_1$, $\beta_2$ and $\epsilon$ must be optimized, sometimes independently for each oscillator. However, gradient descent has not been the default approach. Because each oscillator in a GrFNN has a steady state solution, one can set Eq \eqref{eq:5.1} equal to zero to identify the values of $\alpha$, $\beta_1$, $\beta_2$ and $\epsilon$ that result in the desired amplitude and phase of $z$. This analysis reveals solutions with patterns of activity that include Hopf bifucations, limit-cycles, and double limit-cycles \cite{kim2015signal}. Identifying the desired steady state behavior is possible when the input is tractable (i.e. a pure tone, or a song with a steady beat). However, Eq \eqref{eq:5.1} cannot be solved for a steady state when the input is noisy or more difficult to characterize mathematically (i.e. noisy speech). 

In this investigation we ask the question: can we learn the parameters of a GrFNN via an objective function and gradient descent? To answer this question we implemented the GrFNN toolbox in Tensorflow 2 to use automated differentiation via gradient descent with GPU acceleration. This allows for fast computation of ordinary differential equations (ODEs), like GrFNN. Tensorflow has already been proposed as the best tool to simulate large networks of biological neurons with significant gains in computation speed compared to using only python \cite{mohanta2019parallel}. To test our implementation, we carried out two optimization experiments using gradient descent.

In the first experiment we optimize the parameters of a single GrFNN oscillator to show limit-cycle behavior. GrFNN parameters were randomly initialized and optimized via gradient descent using a mean squared error objective that indicated the magnitude of a limit-cycle oscillator.

In the second experiment we optimize the parameters of a GrFNN network to find the best time-frequency mask to clean a noisy speech signal. Convolution of the GrFNN time-frequency mask and a time-frequency transformation of the noisy speech signal resulted in character error rates that beat performance by current deep learning models to clean noisy speech. 

Together, these experiments show that GrFNNs can be optimized via gradient descent. This work also opens the possibility of incorporating GrFNNs into deep learning algorithms. Such hybrid algorithms could improve signal processing via the dynamical properties of GrFNNs and optimization of parameters with gradient descent.

\section{Results}

\subsection{Experiment 1: Gradient descent optimization of a single GrFNN \\ oscillator to show limit-cycle activity} 

In this first experiment we used gradient descent to optimize the parameters of a single GrFNN oscillator. The goal was to obtain parameters that would make the oscillator show limit-cycle activity. GrFNN oscillators showing limit-cycle activity are well understood \cite{kim2015signal}. The previous analysis by Kim and Large reveals that any GrFNN oscillator with $\alpha>0$, $\beta_1<0$, $beta_2=0$ and $\epsilon=0$ will show a limit-cycle \cite{kim2015signal}. Additionally, if $|\alpha|=|\beta_1|$, the limit-cycle will be unit magnitude. The goal of this experiment is to use gradien descent to find the GrFNN parameters that will result in a unit magnitude limit-cycle in a single oscillator. Fig \ref{f5_1} shows the results. Fig \ref{f5_1}A shows the values of the trainable parameters over the gradient descent iterations. At the beginning, all the values start around zero because the values were drawn from a small normal distribution. As the gradient descent iterations progress, the values of $\alpha$ and $\beta_1$ become positive and negative, respectively. Additionally, because we optimized the limit-cycle to be unit magnitude, the values of $\alpha$ and $\beta_1$ are similar in magnitude. In contrast, the values of $\beta_2$ and $\epsilon$ stay around zero. Fig \ref{f5_1}B and Fig \ref{f5_1}C show the activity of the single GrFNN oscillator at the beginning and the end of the gradient descent process, respectively. Throughout the duration of the simulation shown in Fig \ref{f5_1}B, the oscillator shows the magnitude originally drawn from the normal distribution. In contrast, Fig \ref{f5_1}C shows activity that corresponds to a unit magnitude limit-cycle steady state, resulting from the values of $\alpha$ and $\beta_1$ that were learned through gradient descent in this experiment. 

\begin{figure}
    \centering
    \includegraphics[width=1.0\textwidth]{figures/fig5_1.png}
    \caption[Gradient descent optimization process and resulting activity for a single GrFNN oscillator]{\textbf{Gradient descent optimization process and resulting activity for a single GrFNN oscillator.} (A) the values of the trainable parameters $\alpha$, $\beta_1$, $\beta_2$ and $\epsilon$ that were learned over 100 gradient descent iterations. Because the oscillator learns limit-cycle activity, $\alpha$ and $\beta$ become positive and negative, respectively, while $beta_2$ and $\epsilon$ remain around their initial values close to zero (the green line corresponding to $beta_2$ is occluded by the red line that corresponds to $\epsilon$). The oscillator also learns unit magnitude activity, hence why $\alpha$ and $beta_1$ have similar magnitude. (B) The activity shown by the oscillator in a simulation lasting 10 seconds with the parameter values before gradient descent optimization. The blue and orange lines correspond to the real and imaginary parts of the GrFNN oscillator, respectively. This oscillator with parameter values $\alpha$, $\beta_1$, $\beta_2$ and $\epsilon$ that are very close to zero is equivalent to $\frac{1}{f}\dot{z}=z(i2\pi)$ (compare with Eq \eqref{eq:5.1}), which is an oscillator with sustained magnitude always matching its initial magnitude and frequency dictated by $f$. (C) The activity shown by the oscillator in a simulation lasting 10 seconds with the parameter values learned via gradient descent optimization. The oscillator shows unit magnitude limit-cycle activity.} 
    \label{f5_1}
\end{figure}

This initial experiment shows that it is possible to find the parameters of a single GrFNN oscillator via gradient descent. As long as a well-described steady state is known in advance, iterative gradient descent is a plausible method to find the GrFNN parameters that will result in an oscillator showing such steady state.

\subsection{Experiment 2: Gradient descent optimization of a GrFNN network \\ for speech enhancement.} 

Experiment 1 showed that we can optimize the parameters of a single GrFNN oscillator to show a specific steady state behavior. In this second experiment we tested whether we could learn GrFNN parameters for different oscillators in a network that is used enhance noisy speech. A GrFNN network can process a noisy signal, resulting in oscillators resonating with the signal but not with the noise. In our second experiment, the GrFNN network processed a noisy speech signal to generate a time-frequency mask, which then could be convolved with the original noisy speech to obtain a cleaned version. Fig \ref{f5_2}A shows the processing pipeline where noisy speech is processed by the GrFNN and the noisey speech time-frequency transformation is also calculated. After that step, the time-frequency transformation and the GrFNN output can be combined to obtain a cleaned signal. Fig \ref{f5_2}B shows the results, comparing character error rate (CER) performance of our model against the Speech Enhancement Generative Adversarial Network (SEGAN), which is a state-of-the-art method for speech enhancement \cite{pascual2017segan}. Our model outperforms SEGAN in the CER metric. 

\begin{figure}
    \centering
    \includegraphics[width=1.0\textwidth]{figures/fig5_2.png}
    \caption[Speech enhancement with a GrFNN network optimized via gradient descent]{\textbf{Speech enhancement with a GrFNN network optimized via gradient descent.} (A) Model used for speech enhancement. The noisy speech signal is processed by a GrFNN network with double limit-cycle units that separate speech and noise to generate a time-frequency mask. In parallel, the time-frequency representation of the noisy signal is computed. Next, the GrFNN mask and the noisy time-frequency representation are combined to yield the enhanced speech. (B) CER performance of noisy speech, speech enhanced by SEGAN, and speech enhanced by our GrFNN model. The GrFNN outperforms SEGAN in the CER metric.}
    \label{f5_2}
\end{figure}

This second experiment shows that the parameters of different oscillators in a GrFNN network can be optimized simultaneously via gradient descent. In contrast with experiment 1 where no input signal was used and the steady state of the network was the training objective, in experiment 2 the GrFNN input was a dataset of speech utterances and the objective was the mean squared error between the enhanced speech and the ground-truth clean speech (see the methods section for more details that differentiated our two experiments). 

\section{Discussion}

\subsection{GrFNN parameters can be optimized with gradient descent}

Our new implementation of GrFNNs with tensorflow 2 allowed us to optimize GrFNN parameters with gradient descentand automatic differentiation in two very different experiments. These two experiments independently covered optimization of a single oscillator and a network of oscillators with and without stimulation, respectively. In the next sections we discuss the implications of these experiments.

\subsection{Experiment 1: Gradient descent optimization of a single GrFNN \\ oscillator to show limit-cycle activity} 


This simple experiment showed that the parameters of a single oscillator can be optimized with gradient descent to achieve a desired steady state behavior. Our results showed that the parameters required for a GrFNN to show unit magnitude limit-cycle behavior can be found through gradient descent if the objective function compares the amplitude of the oscillator with a desired limit-cycle amplitude. Results showed that $\alpha \approx 0.2$ and $\beta_1 \approx -0.2$ had similar values but with opposite signs, as expected \cite{kim2015signal}. What was not necessarily expected was the specific values to be around $0.2$. However, one can understand that this value is related to the time-constant with which the oscillator reaches the unit-magnitude amplitude from its initial conditions in Fig \ref{f5_2} (see \cite{kim2015signal} for a description of the time constant dynamics in a GrFNN oscillator). Because the simulation lasted 10 seconds, the values of alpha needed to be around $0.2$ to ensure that the oscillator would reach the desired behavior by the end of the simulation. A simulation longer than 10 seconds would have resulted in a smaller magnitude of the $\alpha$ and $\beta_1$ parameters, while the opposite would have happened in a shorter stimulation. The other two parameters ($\beta_2$ and $\epsilon$) would have remained around zero because they are not relevant to the limit-cycle behavior.

Similar examples can be carried out to find parameters for all other possible behaviors that a GrFNN oscillator can show, including hopf bifurcation, limit-cycle, and double limit-cycles (see \cite{kim2015signal} for a complete description of possible states). Additionally, other parameters can be found through gradient descent. For example, if a stimulus exists, the stimulus amplitude to achieve a desired steady state could be found. Additionally, the frequency parameter of the oscillator could also be found in case the desired steady state activity has a specific frequency of activity. This first experiment demonstrates that gradient descent can optimally identify the parameters associated with steady states that are usually found by hand through stability analysis of a dynamical system. 

\subsection{Experiment 2: Gradient descent optimization of a GrFNN network \\ for speech enhancement.} 

This second experiment demonstrates that the parameters of a GrFNN network can be identified with gradient descent. The objective function did not directly compare the GrFNN activity with a target, like in experiment 1. Instead, the GrFNN output was further processed to obtain an audio signal that was compared against another ground truth audio signal. Hence, this experiment shows that the activity of a GrFNN can be transformed by other computations in order to produce a higher order, more abstract output. As a result, this second experiment also presents GrFNNs as a signal processing tool that can be optimized and combined with other operations. With this experiment we have showed that GrFNN parameters can be found using gradient descent with a data-driven and result-focused goal.

\subsection{General discussion}

We have presented a new toolbox to optimize GrFNNs with gradient descent in tensorflow 2. This new implementation opens the doors to expand the applications and topologies of GrFNNs. This was previously impractical due to the required steady state analysis to find parameters for all GrFNN oscillators in a complex model. Now gradient descent can do the parameter finding as long as we know which behavior we want to see from single oscillators or from a GrFNN network. Tensorflow also allows us to further process the GrFNN activity with mathematical functions and train GrFNNs parameters to generate abstract outputs typically used in machine learning, like classification categories or regression. A natural consequence of this toolbox is to use GrFNNs as time-frequency analyzers of signals in deep learning architectures. Because of their similarities to gammatone-filters and their tonotopy, GrFNNs could substitute time-frequency analysis methods like FFT, wavelet transforms, MFCCs, and mel filter-banks, all of which are usually the first step when processing raw audio in a deep learning architecture \cite{purwins2019deep}.

This toolbox will also allow for the tensorflow implementation of networks of oscillatory models other than GrFNNs. Our runge-kutta integrator can integrate any ODE, and models relevant to neuroscience and biophysics are ODEs. We believe that our toolbox is the first step toward building a more robust and general tool for simulation of artificial biological systems. Additionally, other integrators (perhaps more appropriate for different types of models) could be written in addition to the fourth order runge kutta integrator that we implemented. The resulting simulations from our toolbox will have theoretical and practical applications in academic and industry settings. 

\section{Methods}

\subsection{Technical background}

Tensorflow was released in 2015 by Google as a free and open-source toolbox to build and train neural networks (and other kinds of machine learning models) \cite{abadi2016tensorflow}. Tensorflow allows for automatic differentiation of functions with respect to parameters, so that parameters can be updated and the function outputs match predetermined target values. To make this process as efficient as possible, tensorflow supports seamless GPU acceleration of these operations. Since its launch, communities of developers have contributed numerous features to tensorflow, primarily related to advances in deep learning research, allowing tensorflow to constantly be up to date with the most recent breakthroughs in artificial intelligence \cite{abadi2016tensorflow}. Tensorflow is currently in its second version (tensorflow 2.0, released in 2019), which provides multiple improvements over the previous version, including eager execution of code without needing a fixed graph and better GPU performance \cite{campesato2019tensorflow}.

While tensorflow was designed to design machine learning algorithms, one can compute any mathematical function using tensorflow. Considering this, Mohanta and Assisi \cite{mohanta2019parallel} proposed using tensorflow to streamline modeling of biophysical equations (usually ODEs or partial differential equations). Their idea is particularly motivated by the fact that in tensorflow one can define large networks of interacting units, or neurons, and such networks are automatically computed with GPU acceleration (assuming the right hardware and drivers are installed) without having to deal with any GPU code \cite{mohanta2019parallel}. They simulated a realistic model of an insect’s olfactory system consisting of multiple types of neurons connected by different types of synapses \cite{mohanta2019parallel}. The results by Mohanta and Assisi \cite{mohanta2019parallel} demonstrate that one can use tensorflow as a scalable tool to compute a network consisting of thousands of coupled biological neurons (conductance-based models in their specific examples).

\subsection{Implementation}

Given that GrFNNs are ODEs and are originally derived from a model of neural oscillatory activity, we built upon Mohanta and Assisi's work \cite{mohanta2019parallel} to implement GrFNNs in tensorflow 2. We wrote a fourth order runge-kutta integrator that solves any arbitrary ODE by converting it into a tensorflow graph. All GrFNN parameters can be defined as tensorflow variables for every oscillator, and arbitrary parametric connections (static or ODE) can be specified between oscillators or even different GrFNN networks. The output activity of a GrFNN model can be further transformed by an objective function in order to compute an error term that can be used to optimize the values of GrFNN parameters. 

\subsection{Experiment 1: Gradient descent optimization of a single GrFNN \\ oscillator to show limit-cycle activity} 

\subsubsection{Task}

The goal of this experiment was to optimize the parameters of a single GrFNN oscillator to show unit magnitude limit-cycle activity at a frequency of 1Hz in a simulation that lasted 10 seconds. 

\subsubsection{Setup, procedure, and optimization}

We used our GrFNN implementation in tensorflow to define a single GrFNN oscillator like the one in Eq \eqref{eq:5.1}. The trainable parameters (or tensorflow variables) were $\alpha$, $\beta_1$, $\beta_2$, and $\epsilon$, all of which were initialized to be small random values close to zero. $\alpha$ and $\beta_1$ were drawn from a zero centered uniform distribution ranging from $-0.00001$ to $0.00001$. $\beta_2$ and $\epsilon$ are meant to be negative and positive, respectively \cite{kim2015signal}, so they were separately drawn from two uniform distributions, one ranging from $-0.00001$ to $0$ and the other one ranging from $0$ to $0.00001$. The frequency of the oscillator was fixed to be $f=1$, and the oscillator's initial conditions were a real and an imaginary number independently drawn from a gaussian distribution with mean of zero and variance of 1. In this simulation there was no stimulus (i.e. $x(t)=0$ in Eq \eqref{eq:5.1}). The resulting simulation was defined to last 10 seconds with a timestep of 0.05 seconds. To optimize the parameters, the oscillator was first integrated using the fourth order runge-kutta and the magnitude of the GrFNN oscillator during the last 6 timesteps was compared with an array of ones via mean squared error. Then, tensorflow automatically computed the gradients for the trainable parameters $\alpha$, $\beta_1$, $\beta_2$, and $\epsilon$, and updated its values using vanilla gradient descent. This procedure was repeated 100 times. 

\subsection{Experiment 2: Gradient descent optimization of a GrFNN network \\ for speech enhancement.} 

\subsubsection{Task}

In this experiment we optimized the parameters of a network of GrFNN oscillators to find the time-frequency mask that separated speech from pink noise.

\subsubsection{Setup, procedure, and optimization}

We defined a network of 78 GrFNN oscillators with frequencies logarithmically spaced from 64Hz to 8192Hz. The trainable parameters for each oscillator were $\alpha$, $\beta_1$, $\beta_2$, and $\epsilon$, which were initialized to be parameters associated with double limit-cycle dynamics (see \cite{kim2015signal}). The frequency of the oscillators was fixed, and all oscillators were initialized to be zero. The stimulus was a subset of 48 utterances from the TIMIT dataset \cite{garofolo1993timit} mixed with pink noise and a signal-to-noise ratio of 0dB. The resulting GrFNN simulations matched the length of the longest utterance in our dataset (all utterances were zero padded to match the longest utterance length) and had a sampling rate of 22050. All utterances were processed by the GrFNN, resulting in a time-frequency mask which then was multiplied element-wise with the time-frequency transformation of the corresponding noisy utterance. The resulting time-frequency map was converted back to the time domain, and the mean squared error between the GrFNN-processed utterance and the ground-truth clean version was computed. Tensorflow automatically found the gradient of all model parameters and used the gradient to optimize the parameter values. This process was repeated 200 epochs with an ADAM optimizer \cite{kingma2014adam} to find the best parameters. The 48 TIMIT utterances were run through the SEGAN model for speech enhancement to have a baseline comparison \cite{pascual2017segan}. Next, the noisy, the GrFNN enhanced, and the SEGAN enhanced versions of the TIMIT utterances were run through the DeepSpeech model \cite{hannun2014deep} to obtain character-based transcriptions. These transcriptions were compared against the DeepSpeech transcription of the clean version of the TIMIT utterances to assess the character error rate. 


\chapter{Conclusion}

\section{Summary of findings}
The three main studies presented in this dissertation were described in chapters 2, 3, and 5. The following subsections summarize the main findings of each of these chapters.

\subsection{Chapter 2}
This model showed that strong anticipation can explain human anticipatory tendencies when synchronizing movements with a metronome. Specifically, an oscillatory model with delayed feedback resulted in anticipatory behavior when synchronizing with a periodic stimulus. Moreover, two different amplitudes for the delayed feedback term allowed us to simulate the different anticipatory tendencies observed in musicians and non-musicians. The smaller delayed feedback amplitude simulated the musician anticipation and a larger delayed feedback amplitude simulated the non-musician anticipation. 
    
The same model and the same parameters were used to simulate different musical behaviors in solo and group settings. One of the behaviors involved transmission delays, which cancel out the anticipatory tendency due to the extra time it takes for two synchronizing parties to hear each other's actions. The model could also make predictions about human behavioral data not yet collected, specifically for tasks where musician data exists, but non-musician data is still missing. Thus, our model predictions can be validates in the future by collecting this missing data.

\subsection{Chapter 3}
This model demonstrated how the spontaneous motor tempo (SMT) systematically affects the tempo of musicians performing simple melodies. Our oscillatory model simulated the SMT with the oscillator natural tempo (ONT). We used Hebbian learning to adaptively adjust the model tempo and match the stimulus tempo. Additionally, the model also had an elastic force constantly pulling the model tempo to its ONT. This resulted in our model anticipating stimuli slower than the ONT, and lagging stimuli faster than the ONT. Additionally, in the absence of a stimulus, the model had a tendency to return to its ONT. Finally, when two of these models synchronized with each other, the difference between the two models' ONTs was proportional to the absolute asynchrony observed between the models.

We used the same model and the same set of parameters to simulate human tasks previously described in the literature. All our simulations and observations made with our model were consistent with the existing human data. Additionally, we also used our model to make predictions about human data that has not yet been collected, and these predictions can be tested in the future by collecting the missing human data.
    
\subsection{Chapter 5}
We implemented the first tensorflow library of neural oscillators, allowing for optimization of oscillatory networks parameters via automatic differentiation and gradient descent. We observed that using gradient descent optimization one can identify model parameters for well-known oscillatory dynamics like limit-cycles. Additionally, one can use gradient descent to find parameters for oscillatory networks using an objective function and a data-driven approach.

\section{Contributions}
This dissertation ultimately has a set of clear contributions to the fields of dynamical systems modeling of human behavior and signal processing. 

\subsection{Strong anticipation in human behavior}
We have demonstrated that one can use a dynamical system with delayed feedback to simulate human anticipation during perception-action coordination. We only needed to add delayed feedback to an oscillator in order to explain human anticipation. A more complex mechanism of anticipation, like statistical inference, was not necessary. Knowing that delayed feedback causes anticipation tendencies in humans means that we can hypothetically measure axonal delays and make predictions about anticipatory behavior. Conversely, we can also hypothetically measure anticipation tendencies and predict a person’s axonal delays. 

Using the same model we simulated human anticipation in solo and interpersonal perception action coordination tasks. We explained 3 very different experiments using the same set of parameters. Our model is simple and parsimonious because it uses an oscillator that has previously been studied in the literature, and we only added a delay to this oscillator in order to explain anticipacion. 

\section{Elastic Hebbian tempo learning}
We have identified the mechanisms that relate the spontaneous motor tempo (SMT) to tempo-keeping behavior in humans. The mechanisms are adaptive tempo learning and an elastic force constantly pulling to an individual’s SMT. Similar to the strong anticipation model, this model also accounts for a variety of synchronizing behaviors observed in real human data in solo and interpersonal settings.

\section{Gradient descent optimization of oscillator networks}
Our implementation will allow for data-driven and large-scale optimization of oscillatory networks as well as other dynamical system models from biophysics. This new implementation will also allow for the integration of oscillatory networks in deep learning and machine learning algorithms. In the specific case of Gradient Frequency Neural Networks, their dynamics could lead to major advancements in the spectral processing of signals in the feature extraction layers of common deep learning algorithms for speech and music applications


\appendix
\chapter{Supplementary figures}
\begin{figure}
    \centering
    \includegraphics[width=1.0\textwidth]{figures/figS_1.png}
    \caption[The SAPPA model's behavior when the external periodic input is a square wave instead of a sinusoid]{\textbf{The SAPPA model's behavior when the external periodic input is a square wave instead of a sinusoid.} (A) Illustration of what the asynchrony between the SAPPA model and the external square wave stimulus looks like, and how it's measured. (B) Analysis of the asynchrony (in ms) as a function of $D$ and $\tau$ in Eq \eqref{eq:2.5} when $A = -0.5$ and $f = 1$. (C) The anticipation observed when the musician (green dots) and non-musician (yellow dots) SAPPA models were stimulated by the external square wave while also receiving their own non-delayed activity as input ($A = -0.5$). In all simulations $\tau = 0.222$ seconds. The $D$ parameter differentiates the musician and non-musician models. The regression lines for the behavioral data originally shown in Fig \ref{f2_1}A are shown for comparison purposes in (C).}
    \label{fS_1}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=1.0\textwidth]{figures/figS_2.png}
    \caption[Analysis of the asynchrony between the SAPPA model and the stimulus as a function of the recurrent delay $\tau$]{\textbf{Analysis of the asynchrony between the SAPPA model and the stimulus as a function of the recurrent delay $\tau$.} Rectangular plots (left panels) show the asynchrony as a function of $\tau$ (in units of seconds) for different values of $A$ while $D$ stays constant ($D = 1.0$; $f = 1.0$). Circular plots (right panels) show the angle of the asynchrony and the magnitude of the SAPPA model. In the rectangular plots, asynchrony is shown in units of radians, and not in seconds, in order to match the cyclic dynamic range of the circular plots. In the rectangular plots, gray-shaded areas indicate regions where the SAPPA model did not synchronize with the stimulus, and instead mode-locking was observed. Vertical dotted lines indicate values of $\tau$ for which circular plots were calculated. In the circular plots, individual blue lines start from different initial conditions, all of which arrive to either a red dot (a fixed point) or a red ring (a limit cycle). (A) $A = -1.0$ (B) $A = -0.5$ (C) $A = 0.0$ (D) $A = 0.5$ (E) $A = 1.0$. The limit cycle behavior is observed when the SAPPA model does not synchronize with the stimulus, and instead the SAPPA model mode-locks with the stimulus. Note: the circular plots are known as polar plots.}
    \label{fS_2}
\end{figure}

\bibliographystyle{unsrt}
\bibliography{mybib}
\end{document}
